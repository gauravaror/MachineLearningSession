We introduce a neural semantic parser which is interpretable and scalable. The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are different from linguistically motivated ones.1 The semantic parser is trained end-to-end using annotated logical forms or their denotations. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. We achieve the state of the art on SPADES and GRAPHQUESTIONS and obtain competitive results on GEOQUERY and WEBQUESTIONS. Evaluation of the induced predicate-argument relations against syntax-based ones reveals that they are interpretable and meaningful compared to heuristic baselines, but they sometimes deviate from linguistic conventions. In this work we consider five types of domain-general predicates illustrated in Table 1. The objective is to maximize the log likelihood of the correct answer y given x by summing over all grounded candidates G with denotation y (i.e.,[[G]]K = y):
Ly = ∑
(x,y)∈T log
∑
[[G]]K=y
p(G|x) (8)
p(G|x) ∝ exp{f(G, x)} (9) where f(G, x) is a feature function that maps pair (G, x) into a feature vector. We conduct experiments on four datasets, including GEOQUERY (which has logical forms; Zelle and Mooney 1996), SPADES (Bisk et al., 2016), WEBQUESTIONS (Berant et al., 2013), and GRAPHQUESTIONS (Su et al., 2016) (which have denotations). Since creating a manual gold standard for these large datasets is time-consuming, we compared the induced representations against the output of a syntactic parser. For further comparison, we built a simple baseline which identifies predicates based on the output of the Stanford POStagger (Manning et al., 2014) following the ordering VBD VBN VB VBP VBZ MD. In terms of evaluation, we use three metrics shown in Table 7. Our model falls under the second class of approaches where utterances are first mapped to an intermediate representation containing natural language predicates. For WEBQUESTIONS and GRAPHQUESTIONS, we follow the procedure described in Reddy et al. Acknowledgments We would like to thank three anonymous reviewers, members of the Edinburgh ILCC and the IBM Watson, and Abulhair Saparov for feedback. The majority of questions include at most one entity. Assuming for simplicity that domaingeneral predicates share the same vocabulary
in ungrounded and grounded representations, the ungrounded representation for the example utterance is:
answer(exclude(states(all), border(texas)))
where states and border are natural language predicates. If a RED action results in having no more open non-terminals left on the stack, the transition system terminates. 5 Discussion  We presented a neural semantic parser which converts natural language utterances to grounded meaning representations via intermediate predicate-argument structures. The second set of features include the embedding similarity between the relation and the utterance, as well as the similarity between the relation and the question words. SCANNER achieves a new state of the art on this dataset with a gain of 4.23 F1 points over the best previously reported model. We also observed that the model struggles with control and subordinate constructions. Aside from relaxing strict isomorphism, we would also like to perform crossdomain semantic parsing where the first stage of the semantic parser is shared across domains. Xu et al. The successful application of encoder-decoder models (Bahdanau et al., 2015; Sutskever et al., 2014) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Kočiský et al., 2016). 4.2 Implementation Details  Amongst the four datasets described above, GEOQUERY has annotated logical forms which we directly use for training. We can generate the tree with a top-down, depth first transition system reminiscent of recurrent neural network grammars (RNNGs; Dyer et al. (2016) evaluate the effectiveness of four different CCG parsers on the semantic parsing task when varying the amount of supervision required. However, they are real questions asked by people on the Web. When there exist multiple surrogate representations,3 we select one randomly and back-propagate. The sentences include two or more entities and although they are not very compositional, they constitute a large-scale dataset for neural network training. 2 Preliminaries  Problem Formulation Let K denote a knowledge base or more generally a reasoning system, and x an utterance paired with a grounded meaning representationG or its denotation y. Our semantic parser achieves the state of the art on SPADES and GRAPHQUESTIONS, while obtaining competitive results on GEOQUERY and WEBQUESTIONS. The word embeddings were initialized with Glove embeddings (Pennington et al., 2014). RED stands for REDuce and is used for subtree completion. Entity mentions in SPADES have been automatically annotated with Freebase entities (Gabrilovich et al., 2013). To map an ungrounded term ut to a grounded term gt, we compute the conditional probability
of gt given ut with a bi-linear neural network: p(gt|ut) ∝ exp ~ut ·Wug · ~gt> (5) where ~ut is the contextual representation of the ungrounded term given by the bidirectional LSTM, ~gt is the grounded term embedding, and Wug is the weight matrix. Ungrounded Meaning Representation We also use FunQL to express ungrounded meaning representations. For fair comparison, we also built a neural baseline that encodes an utterance with a recurrent neural network and then predicts a grounded meaning representation directly (Ture and Jojic, 2016; Yih et al., 2016). We report accuracy which is defined as the proportion of the utterance that are correctly parsed to their gold standard logical forms. As can be seen, SCANNER outperforms all CCG variants (from unsupervised to fully supervised) without having access to any manually annotated derivations or lexicons. Specifically, we converted the questions to event-argument structures with EASYCCG (Lewis and Steedman, 2014), a high coverage and high accuracy CCG parser. All other embeddings were randomly initialized. The predicate all is a special case which acts as a terminal directly. In order to derive the target logical forms, all we have to do is replacing predicates in the ungrounded representations with symbols in the knowledge base. For the grounded action sequence (which by design is the same as the ungrounded action sequence and therefore the output of the transition system), we can directly maximize the log likelihood log p(a|x) for all examples:
La = ∑
x∈T log p(a|x) =
∑
x∈T
n∑
t=1
log p(at|x) (6)
where T denotes examples in the training data. The output of the transition system is then grounded (e.g., to a knowledge base) with a neural mapping model under the assumption that grounded and ungrounded structures are isomorphic.2 As a result, we obtain a neural model that jointly learns to parse natural language semantics and induce a lexicon that helps grounding. To further analyze how the learned predicates differ from syntax-based ones, we grouped utterances in SPADES into four types of linguistic constructions: coordination (conj), control and raising (control), prepositional phrase attachment (pp), and subordinate clauses (subord). SCANNER performs competitively despite not having access to any linguistically-informed syntactic structures. SCANNER obtains performance on par with the best symbolic systems (see the first block in the table). We report F1 for SCANNER, the neural baseline model, and three symbolic systems presented in Su et al. Results on WEBQUESTIONS are summarized in Table 3. This predicate is either a natural language expression such as border, or one of the domain-general predicates exemplified in Table 1 (e.g., exclude). It has difficulty distinguishing control from raising predicates as exemplified in the utterance ceo john thain agreed to leave from Table 9, where it identifies the raising predicate agreed. As can be seen in Table 8, the matching score is relatively high for utterances involving coordination and prepositional phrase attachments. The surrogates were selected from a subset of candidate Freebase graphs, which were obtained by entity linking. 4 Experiments  In this section, we verify empirically that our semantic parser derives useful meaning representations. However, FunQL is less expressive than lambda calculus, partially due to the elimination of variables. In the absence of anaphora and composite binary predicates, conversion algorithms exist between FunQL and λ-DCS. Note that U is defined by a sequence of actions (denoted
by a) and a sequence of term choices (denoted by u) as shown in Table 2. When only denotations are available, we compare surrogate meaning representations against our predictions (Reddy et al., 2014). A merit of the two-stage approach is that it creates reusable intermediate interpretations, which potentially enables the handling of unseen words and knowledge transfer across domains (Bender et al., 2015). This allows the neural network to focus on parsing and lexical mapping, sidestepping the challenging structure mapping problem which would result in a larger search space and higher variance. In this work, we propose a neural semantic parser that alleviates the aforementioned problems. This is achieved with a transition-based approach which by design yields recursive semantic structures, avoiding the problem of generating ill-formed meaning representations. (2015), which takes as input the concatenated representation of the predicate and argument of the subtree. The semantic parsing task is decomposed in two stages: we first explain how an utterance is converted to an intermediate representation (Section 3.1), and then describe how it is grounded to a knowledge base (Section 3.2). Specifically, the questions were created by randomly removing an entity, thus producing sentence-denotation pairs (Reddy et al., 2014). Their system uses Wikipedia to prune out erroneous candidate answers extracted from Freebase. We also describe implementation details and the features used in the discriminative ranker. As a reminder, the task in SPADES is to predict the entity masked by a blank symbol ( ). We give details on the evaluation datasets and baselines used for comparison. We aim to maximize the likelihood of the grounded meaning representation p(G|x) over all training examples. An assumption our model imposes is that ungrounded and grounded representations are structurally isomorphic. In general, the transition system generates the representation by following a derivation tree (which contains a set of applied rules) and some canonical generation order (e.g., depth-first). Previous work (Kwiatkowski et al., 2013) models such cases by introducing collapsing (for many-to-one mapping) and expansion (for one-to-many mapping) operators. Compared to previous neural semantic parsers, our model is more interpretable as the intermediate structures are useful for inspecting what the model has learned and whether it matches linguistic intuition. Our model
essentially jointly learns how to parse natural language semantics and the lexicons that help grounding. Table 6 reports SCANNER’s performance on SPADES. 3.2 Generating Grounded Representations  Since we constrain the network to learn ungrounded structures that are isomorphic to the target meaning representation, converting ungrounded representations to grounded ones becomes a simple lexical mapping problem. We identify po-
tential entity spans using seven handcrafted partof-speech patterns and associate them with Freebase entities obtained from the Freebase/KG API.4 We use a structured perceptron trained on the entities found in WEBQUESTIONS and GRAPHQUESTIONS to select the top 10 non-overlapping entity disambiguation possibilities. Notice that domain-general predicates are often implicit, or represent extra-sentential knowledge.   Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1005  1 Introduction  Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. The second row refers to the percentage of structure matches, where the predicted representations have the same structure as the human annotations, but may not use the same lexical terms. Finally, GRAPHQUESTIONS (Su et al., 2016) contains 5,166 question-answer pairs which were created by showing 500 Freebase graph queries to Amazon Mechanical Turk workers and asking them to paraphrase them into natural language. Table 2 shows the transition actions used to generate our running example. For instance, Freebase does not contain a relation representing daughter, using instead two relations representing female and child. Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded
1Our code is available at https://github.com/ cheng6076/scanner. (2016). The global effect of the above update rule is close to maximizing the marginal likelihood of denotations, which differs from recent work on weakly-supervised semantic parsing based on reinforcement learning (Neelakantan et al., 2017). Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches. An advantage of this assumption is that tokens in the ungrounded and grounded representations are strictly aligned. It is important to note that Bast and Haussmann (2015) develop a question answering system, which contrary to ours can-
not produce meaning representations whereas Berant and Liang (2015) propose a sophisticated agenda-based parser which is trained borrowing ideas from imitation learning. The model generates the ungrounded representation U conditioned on utterance x by recursively calling one of the above three actions. The support of the European Research Council under award number 681760 “Translating Multiple Modalities into Text” is gratefully acknowledged. The open bracket will be closed by a reduce operation. The last set of features includes the answer type as indicated by the last word in the Freebase relation (Xu et al., 2016). SCANNER yields performance improvements over these
4http://developers.google.com/ freebase/
systems when using comparable data sources for training. For all Freebase experiments, we followed previous work (Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014) in additionally training a discriminative ranker to re-rank grounded representations globally. SPADES (Bisk et al., 2016) contains 93,319 questions derived from CLUEWEB09 (Gabrilovich et al., 2013) sentences. The utterances are compositional, but the language is simple and vocabulary size small. The above grounding step can be interpreted as learning a lexicon: the model exclusively relies on the intermediate representation U to predict the target meaning representation G without taking into account any additional features based on the utterance. EASYCCG extracts predicate-argument structures with a labeled F-score of 83.37%. Compared to most existing semantic parsers which employ a CKY style bottom-up parsing strategy (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Berant et al., 2013; Berant and Liang, 2014), the transition-based approach we proposed does not require feature decomposition over structures and thereby enables the exploration of rich, non-local features. The first row shows the percentage of exact matches between the predicted representations and the human annotations. (2016). However, since the key idea of our model is to capture salient meaning for the task at hand rather than strictly obey syntax, we would not expect the
predicates induced by our system to entirely agree with those produced by the syntactic parser. Moreover, without any taskspecific prior knowledge, the learning problem is fairly unconstrained, both in terms of the possible derivations to consider and in terms of the target output which can be ill-formed (e.g., with extra or missing brackets). Among structurally correct predictions, we additionally compute how many tokens are correct, as shown in the third row. To select a domain-general term, we use the same representation of the transition system et to compute a probability distribution over candidate terms:
p(uGENERALt |a<t, x) ∝ exp(Wp · et) (3)
To choose a natural language term, we directly compute a probability distribution of all natural language terms (in the buffer) conditioned on the stack representation st and select the most relevant term (Jia and Liang, 2016):
p(uNLt |a<t, x) ∝ exp(st) (4)
When the predicted action is RED, the completed subtree is composed into a single representation on the stack. We optimize this objective with the method described in Lei et al. Table 8 also shows the breakdown of matching scores per linguistic construction, with the number of utterances in each type. Jia and Liang (2016) achieve better results with synthetic data that expands GEOQUERY; we could adopt their approach to improve model performance, however, we leave this to future work. The non-terminal is popped as well, after which a composite term representing the entire subtree, e.g., border(texas), is pushed back to the stack. Such knowledge plays a critical role in understand modeling limitations so as to build better semantic parsers. FunQL is a variable-free query language, where each predicate is treated as a function symbol that modifies an argument list. We treat each possibility as a candidate input utterance, and use the perceptron score as a feature in the discriminative reranker, thus leaving the final disambiguation to the semantic parser. As shown in Table 8, on SPADES and WEBQUESTIONS, the predicates learned by our model match the output of EASYCCG more closely than the heuristic baseline. Moreover, some predicates cannot be clearly anchored to a token span. (2016). The first feature is the likelihood score of a grounded representation aggregating all intermediate representations. While the stack representation st is easy to retrieve as the top state of the stack-LSTM, obtaining the buffer representation bt is more involved. Recall that a = [a1, · · · , an] denotes the transition action sequence defining U and G; let u = [u1, · · · , uk] denote the ungrounded terms (e.g., predicates), and g = [g1, · · · , gk] the grounded terms. An advantage of FunQL is that the resulting s-expression encodes semantic compositionality and derivation of the logical forms. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015). This
3The average Freebase surrogate representations obtained with highest denotation match (F1) is 1.4.
likelihood can be decomposed into the likelihood of the grounded action sequence p(a|x) and the grounded term sequence p(g|x), which we optimize separately. For FunQL, a simple solution exists since the representation itself encodes the derivation. 4.3 Results  Experimental results on the four datasets are summarized in Tables 3–6. Specifically, Bisk et al. Grounded Meaning Representation We represent grounded meaning representations in FunQL (Kate et al., 2005) amongst many other alternatives such as lambda calculus (Zettlemoyer and Collins, 2005), λ-DCS (Liang, 2013) or graph queries (Holzschuher and Peinl, 2013; Harris et al., 2013). As in previous experiments, SCANNER outperforms the neural baseline, too. We also evaluated the intermediate representations created by SCANNER on the other three (Freebase) datasets. This buffer representation is then concatenated with the stack representation to form the system representation et. Surrogate representations are those with the correct denotations. A major difference in our semantic parsing scenario is that tokens in the buffer are not fetched in a sequential order or removed from the buffer. Consider utterance x with ungrounded meaning representation U , and grounded meaning representation G. Both U and G are defined with a sequence of transition actions (same for U and G) and a sequence of terms (different for U and G). We present comparisons of our system which we call SCANNER (as a shorthand for SymboliC meANiNg rEpResentation) against a variety of models previously described in the literature. A more compact logical formulation which our method also applies to is λ-DCS (Liang, 2013). But this modeling flexibility comes at a cost since it is no longer possible to interpret how meaning composition is performed. 2016). This property makes FunQL logical forms convenient to be predicted with recurrent neural networks (Vinyals et al., 2015; Choe and Charniak, 2016; Dyer et al., 2016). Similar to SPADES, it is based on Freebase and the questions are not very compositional. Note that the terminal choice does not include variable (e.g., $0, $1), since FunQL is a variable-free language which sufficiently captures the semantics of the datasets we work with. exclude is a predicate that returns the difference between two input sets. Each predicate (e.g., border) can be visualized as a non-terminal node of the tree and each entity (e.g., texas) as a terminal. However, rather than using an external parser (Reddy et al., 2014, 2016) or manually specified CCG grammars (Kwiatkowski et al., 2013), we induce intermediate representations in the form of predicate-argument structures
44
from data. Apart from the entity score, the discriminative ranker uses the following basic features. (2016) represent the state of the art on WEBQUESTIONS. It recursively pops elements from the stack until an open non-terminal node is encountered. For the other three datasets, we treat surrogate meaning representations which lead to the correct answer as gold standard. For example, the FunQL representation for the utterance which states do not border texas is:
answer(exclude(state(all), next to(texas)))
where next to is a domain-specific binary predicate that takes one argument (i.e., the entity texas) and returns a set of entities (e.g., the states bordering Texas) as its denotation. WEBQUESTIONS (Berant et al., 2013) contains 5,810 question-answer pairs. Similar to RNNG, our
algorithm uses a buffer to store input tokens in the utterance and a stack to store partially completed trees. All previous neural systems (Dong and Lapata, 2016; Jia and Liang, 2016) treat semantic parsing as a sequence transduction problem and use LSTMs to directly map utterances to logical forms. The experimental setup aims to shows how humans can participate in improving the semantic parser with feedback at the intermediate stage. For example, the predicate all in the above utterance represents all states in the domain which are not mentioned in the utterance but are critical for working out the utterance denotation. representation (Kwiatkowski et al., 2013; Reddy et al., 2016, 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Therefore, we allow the generation algorithm to pick tokens and combine logical forms in arbitrary orders, conditioning on the entire set of sentential features. But we do not find this helpful in experiments. We therefore compute at each time step an adaptively weighted representation of bt (Bahdanau et al., 2015) conditioned on the stack representation st. Consider again answer(exclude(states(all), border(texas))) which is tree structured. For the choice of composition function, we use a single-layer neural network as in Dyer et al. GEOQUERY (Zelle and Mooney, 1996) contains 880 questions and database queries about US geography. The first block contains symbolic systems, whereas neural models are presented in the second block. The second block in Table 3 reports the results of several neural systems. TER(X) generates a TERminal entity or the special predicate all. Such models still fall under the first approach, however, in contrast to previous work (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011) they reduce the need for domain-specific assumptions, grammar learning, and more generally extensive feature engineering. Our problem is to learn a semantic parser that maps x to G via an intermediate ungrounded representation U . Our model would also benefit from a similar post-processing step. Again, we observe that SCANNER outperforms this baseline. Finally, Table 4 presents our results on GRAPHQUESTIONS. The framework could be extended to generate directly acyclic graphs by incorporating variables with additional transition actions for handling variable mentions and co-reference. For subordinate clauses, SCANNER tends to take shortcuts identifying as predicates words closest to the blank symbol. This is because we do not have an explicit buffer representation due to the non-projectivity of semantic parsing. The latter consist primarily of natural language predicates and domain-general predicates. The whole network is trained end-to-end on natural language utterances paired with annotated logical forms or their denotations. When G is executed against K, it outputs denota-
2We discuss the merits and limitations of this assumption in Section 5
tion y. In this case, we additionally rely on a discriminative reranker which ranks the grounded representations derived from ungrounded representations (see Section 3.4). However, we leave this to future work. (2016) who also learn a semantic parser via intermediate representations which they generate based on the output of a dependency parser. The discriminative ranker is a maximumentropy model (Berant et al., 2013). For simplicity, hereafter we do not differentiate natural language and domain-general predicates. 4.4 Analysis of Intermediate Representations  Since a central feature of our parser is that it learns intermediate representations with natural language predicates, we conducted additional experiments in order to inspect their quality. The type of predicate is determined by the placeholder X and once generated, it is pushed onto the stack and represented as a non-terminal followed by an open bracket (e.g., ‘border(’). The conditional probability p(U |x) is factorized over time steps as:
p(U |x) = p(a, u|x)
=
T∏
t=1
p(at|a<t, x)p(ut|a<t, x)I(at 6=RED) (1)
where I is an indicator function. When the predicted action is either NT or TER, an ungrounded term ut (either a predicate or an
entity) needs to be chosen from the candidate list depending on the specific placeholder X. As can be seen, the induced meaning representations overlap to a large extent with the human gold standard. SCANNER is conceptually similar to Reddy et al. At each time step, the model uses the representation of the transition system et to predict an action:
p(at|a<t, x) ∝ exp(Wa · et) (2)
where et is the concatenation of the buffer representation bt and the stack representation st. For all Freebase related datasets we use average F1 (Berant et al., 2013) as our evaluation metric. We give details on the features we used in Section 4.2. Alternative solutions in the traditional semantic parsing literature include a floating chart parser (Pasupat and Liang, 2015) which allows to construct logical predicates out of thin air. To predict the actions of the transition system, we encode the input buffer with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and the output stack with a stack-LSTM (Dyer et al., 2015). The model will often identify informative predicates (e.g., nouns) which do not necessarily agree with linguistic intuition. For example, in the utterance wilhelm maybach and his son started maybach in 1909 (see Table 9), SCANNER identifies the predicateargument structure son(wilhelm maybach) rather than started(wilhelm maybach). all is a special predicate that returns a collection of entities. In practice, U may provide sufficient contextual background for closed domain semantic parsing where an ungrounded predicate often maps to a single grounded predicate, but is a relatively impoverished representation for parsing large open-domain knowledge bases like Freebase. Our transition system defines three actions, namely NT, TER, and RED, explained below. Within our current framework, these two types of structural mismatches can be handled with semi-Markov assumptions (Sarawagi and Cohen, 2005; Kong et al., 2016) in the parsing (i.e., predicate selection) and the grounding steps, respectively. We used the Adam optimizer for training with an initial learning rate of 0.001, two momentum parameters [0.99, 0.999], and batch size 1. The dimensions of the word embeddings, LSTM states, entity embeddings and relation embeddings are [50, 100, 100, 100]. NT(X) generates a Non-Terminal predicate. In this work we constrain ungrounded representations to be structurally isomorphic to grounded ones. In Table 9, we provide examples of predicates identified by SCANNER, indicating whether they agree or not with the output of EASYCCG. But for GRAPHQUESTIONS which contains more compositional questions, the mismatch is higher. GEOQUERY results are shown in Table 5. 4.1 Datasets  We evaluated our model on the following datasets which cover different domains, and use different types of training data, i.e., pairs of natural language utterances and grounded meanings or question-answer pairs. On the negative side, the structural isomorphism assumption restricts the expressiveness of the model, especially since one of the main benefits of adopting a two-stage parser is the potential of capturing domain-independent semantic information via the intermediate representation. 3.1 Generating Ungrounded Representations  At this stage, utterances are mapped to intermediate representations with a transition-based algorithm. 3.3 Training Objective  When the target meaning representation is available, we directly compare it against our predictions and back-propagate. While it would be challenging to handle drastically non-isomorphic structures in the current model, it is possible to perform local structure matching, i.e., when the mapping between natural language and domainspecific predicates is many-to-one or one-to-many. For the grounded term sequence g, since the intermediate ungrounded terms are latent, we maximize the expected log likelihood of the grounded terms ∑ u [p(u|x) log p(g|u, x)] for all examples, which is a lower bound of the log likelihood log p(g|x): Lg = ∑
x∈T
∑
u
[p(u|x) log p(g|u, x)]
= ∑
x∈T
∑
u
[ p(u|x) k∑
t=1
log p(gt|ut) ] (7)
The final objective is the combination of La and Lg, denoted as LG = La + Lg. This is because the lexical alignment between an utterance and its semantic representation is hidden. For GEOQUERY
which contains only 280 test examples, we manually annotated intermediate representations for the test instances and evaluated the learned representations against them. 3 Modeling  In this section, we discuss our neural model which maps utterances to target logical forms. Finally, note that for certain domain-general predicates, it also makes sense to extract natural language rationales (e.g., not is indicative for exclude). 3.4 Reranker  As discussed above, for open domain semantic parsing, solely relying on the ungrounded representation would result in an impoverished model lacking sentential context useful for disambiguation decisions. Specifically, they can shed light on the kinds of representations (especially predicates) useful for semantic parsing. A side-product of our modeling framework is that the induced intermediate representations can contribute to rationalizing neural predictions (Lei et al., 2016). Previous work on this dataset has used a semantic parsing framework similar to ours where natural language is converted to an intermediate syntactic representation and then grounded to Freebase. 