Relation extraction has been widely used for finding unknown relational facts from the plain text. Most existing methods focus on exploiting mono-lingual data for relation extraction, ignoring massive information from the texts in various languages. To address this issue, we introduce a multi-lingual neural relation extraction framework, which employs monolingual attention to utilize the information within mono-lingual texts and further proposes cross-lingual attention to consider the information consistency and complementarity among cross-lingual texts. The source code of this paper can be obtained from https://github. Experimental results on real-world datasets show that our model can take advantage of multi-lingual texts and consistently achieve significant improvements on relation extraction as compared with baselines. com/thunlp/MNRE According to the distant supervision data in our experiments3, we find that over half of Chinese
3The data is generated by aligning Wikidata with Chinese
34
and English sentences are longer than 20 words, in which only several words are related to the relational facts. It indicates that our framework can take full advantages of sentences in different languages and better capture sophisticated patterns expressing relations. Formally, the cross-lingual representation Sjk is defined as a weighted sum of those sentence vectors xij in the jth language:
Sjk = ∑
i
αijkx i j , (6)
where αijk is the cross-lingual attention score of each sentence vector xij with respect to the kth language. For instance, though New York and United States are expressed as纽约 and美国 respectively in Chinese, both Americans and Chinese share the fact that “New York is a city of USA.”
It is straightforward to build mono-lingual RE systems separately for each single language. Similarly, by jointly training through multi-lingual attention, MNRE(CNN)-En and MNRE(CNN)-Zh both achieve promising results. To measure the effect of the two relation matrices, we compare the performance of MNRE using the both matrices with those only using M (MNRE-M) and only using R (MNRE-R). It demonstrates that both Chi-
nese and English relation extractors can take full advantages of texts in both languages via our propose multi-lingual attention scheme. (2015) introduce neural networks to extract relations with automatically learned features from training instances. For learning, we iterate by randomly selecting amini-batch from the training set until converge. We take all vectors {Sjk} together and define the overall score function f(T, r) as follows:
f(T, r) = ∑
j,k∈{1,...,m} log p(r|Sjk, θ), (9)
where p(r|Sjk, θ) is the probability of predicting the relation r conditional on Sjk, computed using
a softmax layer as follows:
p(r|Sjk, θ) = softmax(MSjk + d), (10)
where d ∈ Rnr is a bias vector, nr is the number of relation types andM ∈ Rnr×Rc is a global relation matrix initialized randomly. The experimental results show that our framework achieves significant improvement for relation extraction as compared to all baselinemethods including bothmonolingual and multi-lingual ones. Sentence Encoder. Hence, the word-level multi-lingual attention, which may discover implicit alignments between words in multiple languages, will further improve multi-lingual relation extraction. 4.1 Datasets and Evaluation Metrics  We generate a new multi-lingual relation extraction dataset to evaluate our MNRE framework. 3 shows the aggregated precision/recall curves of the four models for both CNN and PCNN. We evaluate our framework onmulti-lingual relation extraction task, and the results show that our framework can effectively model relation patterns among languages and achieve state-of-the-art results. We further give an example of cross-lingual at-
tention in Table 4. KBs are playing an important role in many AI and NLP applications such as information retrieval and question answering. In the testing phase, since the relation is not known in advance, we will construct different vectors {Sjk} for each possible relation r to compute f(T, r) for relation prediction. (2016) further utilize sentence-level attention mechanism to consider all informative sentences jointly. From Fig. Note that, in the training phase, the vectors {Sjk} are constructed using Eq. Here, instead of max pooling operation, we can use piecewise max pooling operation adopted by PCNN (Zeng et al., 2015) which is a variation of CNN to better capture the relation patterns in the input sentence. (8). Take Table 1 for example. But if so, it won’t be able to take full advantage of diverse information hidden in the data of various languages. There are 176 relations including a special relation NA indicating there is no relation between entities. , x nj j } corresponds to the sentence set in the jth language with nj sentences. To solve the optimization problem, we adopt mini-batch stochastic gradient descent (SGD) to minimize the objective function. }withwi ∈ Rd, where d = da+db×2. Finally, we classify relations according to the global vector aggregated from all sentence vectors weighted by mono-lingual attention and cross-lingual attention. It indicates that utilizing Chinese and English sentences jointly is beneficial to extracting novel relational facts. In experiments, we report the precision/recall curves as the evaluation metric. 2, we have the following observa-
tions:
(1) Both [P]CNN+joint and [P]CNN+share achieve better performances as compared to [P]CNN-En and [P]CNN-Zh. (3) For the relations Father and CountryOfCitizenship of which the sentence number in English and Chinese are not so un-balanced, our MNRE can still improve the performance of relation extraction on both English and Chinese sides. Andwe set both validation and testing sets for Chinese and English parts contain the same facts. Lin
et al. To address this issue, Mintz et al. The mono-lingual vector Sj is computed as a weighted sum of those sentence vectors xij :
Sj = ∑
i
αijx i j , (3)
where αij is the attention score of each sentence vector xij , defined as:
αij = exp(eij)∑ k exp(e k j ) , (4)
where eij is referred as a query-based function which scores how well the input sentence xij reflects its labelled relation r. There are many ways to obtain eij , and here we simply compute ei as the inner product:
eij = x i j · rj . In fact, people describe knowledge about the world using various languages. It indicates that we cannot just use global relation matrix for relation prediction. This indicates that no more useful information can be captured by simply increasing model size. Besides, PCNN+share performs worse than PCNN+joint nearly over the entire range of recall. 4.4 Effectiveness of Complementarity  To demonstrate the effectiveness of considering pattern complementarity among languages, we empirically compare the following methods through held-out evaluation: MNRE for English (MNRE-En) and MNRE for Chinese (MNRE-Zh) which only use the mono-lingual vectors to predict relations, and [P]CNN-En and [P]CNN-Zh models. (2) CNN+share only has similar performance as compared to CNN+joint, even through a bit worse when recall ranges from 0.1 to 0.2. Formally, given two entities, their corresponding sentences in m different languages are defined as T = {S1, S2, . We will explore the following directions as future work: (1) In this paper, we only consider sentence-level multi-lingual attention for relation extraction. It suggests there also exists global consistency of relation patterns among languages which cannot be neglected. We define the objective function as follows:
J(θ) = s∑
i=1
f(Ti, ri), (12)
where s indicates the number of all entity pairs with each corresponding to a sentence set in different languages, and θ indicates all parameters of our framework. Note that, for convenience, we denote those mono-lingual attention vectors Sj as Sjj in the remainder of this paper. To take full consideration of these issues, we propose Multi-lingual Attention-based Neural Relation Extraction (MNRE). Afterwards, to consider the complementarity of all informative sentences in various lan-
Baidu Baike and English Wikipedia articles, which will be introduced in details in the section of experiments. In experiments, we find CNN can achieve a better trade-off between computational efficiency and performance effectiveness. Formally, the ith element of the output vector x ∈ Rdc is calculated as:
[x]j = tanh ( max
i (pij)
) . com/thunlp/MNRE  1 Introduction  People build many large-scale knowledge bases (KBs) to store structured knowledge about the real world, such as Wikidata1 and DBpedia2. 2014CB340501), the National Natural Science Foundation of China (NSFC No. As the development of deep learning, Zeng et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. In this way, we can represent the input sentence as a vector sequence w = {w1,w2, . To further alleviate the wrong labelling problem, Riedel et al. Here we simply define Rk as composed by rk in Eq. We highlight the entity pairs in bold face. . However, it has a sharp decline when the recall reaches 0.25. 3.3 Prediction  For each entity pair and its corresponding sentence set T in m languages, we can obtain m ×m vectors {Sjk|j, k ∈ {1, . Hence we adopt different mono-lingual attentions to deemphasize those noisy sentences within each language. In future, we will extendMNRE to more languages and explore its significance. (2011); Surdeanu et al. (2) MNRE can be flexibly implemented in the scenario of multiple languages, and this paper focuses on two languages of English and Chinese. We will explore the effectiveness of word-level multilingual attention for relation extraction as our fu-
ture work. In Table 3 we show the best setting of all parameters used in our experiments. . Next, we use a validation set to determine the best model parameters and choose the best model via early stopping. 2 Related Work  Recent years KBs have been widely used on various AI and NLP applications.   Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 34–43 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1004
for finding unknown relational facts from the plain text. From the table, we can see that:
(1) For the relation Contains of which the number of English training instances is only 1/7 of Chinese ones, CNN-En gets much worse performance as compared to CNN-Zh due to the lack of training data. (3) and (6) using the labelled relation. Given a sentence x and two target entities, we employ CNN to encode relation patterns in x into a distributed representation x. This work is also funded by the Natural Science Foundation of China (NSFC) and the German Research Foundation(DFG) in Project Crossmodal Learning, NSFC 61621136008 / DFC TRR-169. Experimental results on real-world datasets show that our model can take advantage of multi-lingual texts and consistently achieve significant improvements on relation extraction as compared with baselines. The key idea of cross-lingual attention is to emphasize those sentences which have strong consistency among different languages. 3.2 Multi-lingual Attention  To exploit the information of the sentences from all languages, our model adopts two kinds of attention mechanisms for multi-lingual relation extraction, including: (1) the mono-lingual attention which selects the informative sentences within one language and (2) the cross-lingual attention which measures the pattern consistency among languages. Most existing methods focus on exploiting mono-lingual data for relation extraction, ignoring massive information from the texts in various languages. Acknowledgments  This work is supported by the 973 Program (No. It demonstrates that a simple combination of multiple languages by sharing relation embedding matrices cannot further capture more implicit correlations among various languages. The sentence encoder can also be implemented with GRU (Cho et al., 2014) or LSTM (Hochreiter and Schmidhuber, 1997). . Without loss of generality, the experiments focus on relation extraction from two languages including English and Chinese. For training, we set the iteration number over all the training data as 15. Cross-lingual attention works similar to monolingual attention. To better consider the characteristics of each human language, we further introduce Rk as the specific relation matrix of the kth language. Those vectors with j = k are mono-lingual attention vectors, and those with j ̸= k are cross-lingual attention vectors. (da and db are the dimensions of word embeddings and position embeddings respectively)  3.1.2 Convolution, Max-pooling and Non-linear Layers  After encoding the input sentence, we use a convolutional layer to extract the local features, maxpooling, and non-linear layers to merge all local features into a global representation. 4.5 Comparison of Relation Matrix  For relation prediction, we use two kinds of relation matrices including: M that considers the global consistency of relations, and R that considers the specific characteristics of relations for each language. However, most RE systems concentrate on extracting relational facts frommono-lingual data. Table 5 shows the detailed results (in precision@1) of some specific relations of which the training instances are un-balanced on English and Chinese sides. Fig. From the figure, we observe that:t
(1) The performance of MNRE-M is much worse than both MNRE-R and MNRE. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. Nevertheless, by jointly training through multi-lingual attention, MNRE(CNN)En is comparable to and slightly better than
MNRE(CNN)-Zh. Next, it employs convolutional, max-pooling and non-linear transformation layers to construct the distributed representation of the sentence, i.e., x. 3.2.2 Cross-lingual Attention  Besides mono-lingual attention, we propose crosslingual attention for neural relation extraction to better take advantages of multi-lingual data. The reason is that each language has its own specific characteristics to express relation patterns, which cannot be well integrated into a single relation matrix. The best models were selected by early stopping using the evaluation results on the validation set. The MNRE framework contains two main components:
1. 1http://www.wikidata.org/ 2http://wiki.dbpedia.org/
human-intensive, many works have been devoted to automated extraction of novel facts from various Web resources, where relation extraction (RE) from plain texts is one the most important knowledge sources. The pattern consistency among languages provides us augmented clues to enhance relational pattern learning for relation extraction. Afterwards, we show the effectiveness of our framework of considering pattern complementarity and consistency for multi-lingual relation extraction by quantitative and qualitative analysis. Fig. Different from these works, our framework aims to jointly model the texts in multiple languages to enhance relation extraction with distant supervision. 4 Experiments  We first introduce the datasets and evaluation metrics used in the experiments. (11) used for prediction. With all sentences in various languages encoded into distributed vector representations, we apply mono-lingual and cross-lingual attentions to capture those informative sentences with accurate relation patterns. It is intuitive that each human language has its own characteristics. Moreover, for nearly half of relations, the number of sentences expressing relational facts of these relations varies a lot in different languages. To address this issue, we introduce a multi-lingual neural relation extraction framework, which employs monolingual attention to utilize the information within mono-lingual texts and further proposes cross-lingual attention to consider the information consistency and complementarity among cross-lingual texts. More specifically, for the j-th language and the sentence set Sj , we aim to aggregate all sentence vectors into a real-valued vector Sj for relation prediction. (2012) regard it as a multi-instance multi-label learning problem. We list the statistics about the dataset in Table 2. And we compare the performance of our framework with the [P]CNN model trained with only English data ([P]CNN-En), only Chinese data ([P]CNN-Zh), a joint model ([P]CNN+joint) which predicts using [P]CNN-En and [P]CNN-Zh jointly, and another joint model with shared embeddings ([P]CNN+share) which trains [P]CNN-En and [P]CNN-Zh with common relation embedding matrices. Fortunately, a relational fact is usually expressed with certain patterns in various languages, and the correspondence of these patterns among languages is substantially consistent. From the table we find that, although all of the four sentences actually express the fact that Barzun was born in France, the first and third sentences contain much more noisy information that may confuse RE systems. The reason is perhaps that, CNN-Zh of the relation is not sufficiently trained because there are only 210 Chinese training instances for this relation. 3.1 Sentence Encoder  The sentence encoder aims to transform a sentence x into its distributed representation x via CNN. (2)
The final vector x is expected to efficiently encode relation patterns about target entities from the input sentence. (2015) further employ Universal Schema to combine OpenIE and link-prediction perspective for multi-lingual relation extraction. 3.2.1 Mono-lingual Attention  To address the wrong-labelling issue in distant supervision, we follow the idea of sentence-level attention (Lin et al., 2016) and set mono-lingual attention for MNRE. We follow previous works (Mintz et al., 2009) and investigate the performance of RE systems using the held-out evaluation, by comparing the relational facts discovered by RE systems from the testing set with those facts in KB. We first employ a convolutional neural network (CNN) to embed the relational patterns in sentences into real-valued vectors. Since existing KBs are far from complete and new facts are growing infinitely, meanwhile manual annotation of these knowledge is time-consuming and
∗ Corresponding author: Zhiyuan Liu (liuzy@tsinghua.edu.cn). Most existing RE systems are absorbed in extracting relations from mono-lingual data, ignoring massive information lying in texts from multiple languages. It is thus straightforward that the texts in different languages can be complementary to each other, especially from those resource-rich languages to resource-poor languages, and improve the overall performance of relation extraction. (10) can be extended to:
p(r|Sjk, θ) = softmax[(Rk +M)Sjk + d], (11)
where M encodes global patterns for predicting relations and Rk encodes those language-specific characteristics. . From our experiment data, we also find that 42.2% relational facts in English data and 41.6% ones in Chinese data are unique. (2015) attempt to connect neural networks with distant supervision following the expressed-at-least-once assumption. To address the wrong labelling issue of distant supervision data, Lin et al. Finally, we compare the effect of two kinds of relation matrices in Eq. It shows four sentences having the highest and lowest Chinese-to-English and English-to-Chinese attention weights respectively with respect to the relation PlaceOfBirth in MNRE. Our model measures a score f(T, r) for each relation r, which is expected to be high when r is the valid one, otherwise low. 3.4 Optimization  Here we introduce the learning and optimization details of our MNRE framework. 2. Hence, Eq. In this area, Faruqui and Kumar (2015) present a language independent open domain relation extraction system, and Verga et al. 4.2 Experimental Settings  We tune the parameters of our MNRE framework by grid searching using validation set. (2) Although [P]CNN-En underperforms as compared to [P]CNN-Zh, MNRE-En is comparable to MNRE-Zh by jointly training through multilingual attention. As an important approach to enrich KBs, relation extraction from plain text has attracted many research interests. The source code of this paper can be obtained from https://github. To the best of our knowledge, this is the first effort to multi-lingual neural relation extraction. Many works have been invested to relation extraction including kernelbased model (Zelenko et al., 2003), embeddingbased model (Gormley et al., 2015), CNN-based models (Zeng et al., 2014; dos Santos et al., 2015), and RNN-based model (Socher et al., 2012). We introduce the two components in detail as
follows. Recently, Zeng et al. 3.1.1 Input Representation  Following (Zeng et al., 2014), we transform each input word into the concatenation of two kinds of representations: (1) a word embedding which captures syntactic and semantic meanings of the word, and (2) a position embedding which specifies the position information of this word with respect to two target entities. ,m}} from the neural networks with multi-lingual attention. The held-out evaluation provides an approximate measure of RE performance without time-consuming human evaluation. On the contrary, our proposed MNRE model can successfully improvemulti-lingual relation extraction by considering pattern consistency among languages. , Sm}, where Sj = {x1j , x2j , . The facts in KBs are typically organized in the form of triplets, e.g., (New York, CityOf, United States). The first Chinese sentence has over 20 words, in which only “纽约” (New York) and “ 美国 ” (is the biggest city in the United States) actually directly reflect the relational fact CityOf. For comparison, we also show their attention weights from CNN+Zh and CNN+En. The reason is that those relational facts that are discovered from multiple languages are more reliable to be true. By considering pattern consistency between sentences in two languages with cross-lingual attention, MNRE can identify the second and fourth sentences that unambiguously express the relation PlaceOfBirth with higher attention as compared to CNN+Zh and CNN+En. . In fact, we find that the word alignment information may be also helpful for capturing relation patterns. MNRE further aggregates these sentence vectors with weighted attentions into global representations for relation prediction. In experiments, we build training instances via distant supervision by aligning Wikidata with Chinese Baidu Baike and English Wikipedia articles and evaluate the performance of relation extraction in both English and Chinese. And people speaking different languages also share similar knowledge about the world due to the similarities of human experiences and human cognitive systems. It is thus non-trivial to locate and learn these relational patterns from complicated sentences for relation extraction. Formally, the output of convolutional layer for the ith sliding window is computed as:
pi = Wwi−l+1:i + b, (1)
where wi−l+1:i indicates the concatenation of l word embeddings within the i-th window, W ∈ Rdc×(l×d) is the convolution matrix and b ∈ Rdc is the bias vector. guages and capture the consistency of relational patterns, we apply mono-lingual attention to select the informative sentences within each language and propose cross-lingual attention to take advantages of pattern consistency and complementarity among languages. By grid searching of
4http://iesl.cs.umass.edu/riedel/ecml/
parameters for these baseline models, we can observe that both [P]CNN+joint and [P]CNN+share cannot achieve competitive results compared to MNRE even when increasing the size of the output layer. Suppose j indicates a language and k is a another language (k ̸= j). . From the figure, we find that:
(1) MNRE-En and MNRE-Zh outperform [P]CNN-En and [P]CNN-Zh almost in entire range of recall. . Thus, in this paper, we focus on CNN as the sentence encoder. First, it embeds the words in the input sentence
into dense real-valued vectors. (2) For the relation HeadquartersLocation of which the number of Chinese training instances is only 1/9 of English ones, CNN-Zh even cannot predict any correct results. (3) Our MNRE model achieves the highest precision over the entire range of recall as compared to other methods including [P]CNN+joint and [P]CNN+share models. Multi-lingual Attention. 4.3 Effectiveness of Consistency  To demonstrate the effectiveness of considering pattern consistency among languages, we empirically compare different methods through held-out evaluation. The cross-lingual attention αijk is defined as:
αijk = exp(eijk)∑ k exp(e k jk) , (7)
where eijk is referred as a query-based function which scores the consistency between the input sentence xij in the jth language and the relation patterns in the kth language for expressing the semantic meanings of the labelled relation r. Similar to the mono-lingual attention, we compute eijk as follows:
eijk = x i j · rk, (8)
where rk is the query vector of the relation r with respect to the kth language. The scope of multi-lingual analysis has been widely considered in many tasks besides relation extraction, such as sentiment analysis (Boiy and Moens, 2009), cross-lingual document summarization (Boudin et al., 2011), information retrieval in Web search (Dong et al., 2014) and so on. 2. Hence, we should combine both M and R together for multi-lingual relation extraction, as proposed in our MNRE
framework. 5 Conclusion  In this paper, we introduce a neural relation extraction framework with multi-lingual attention to take pattern consistency and complementarity among multiple languages into consideration. Complementarity. First, the convolutional layer extracts local features by sliding a window of length l over the sentence and perform a convolution within each sliding window. 3 Methodology  In this section, we describe our proposed MNRE framework in detail. In this dataset, the Chinese instances are generated by aligning Chinese Baidu Baike with Wikidata, and the English instances are generated by aligning English Wikipedia articles with Wikidata. The relational facts of Wikidata in this dataset are divided into three parts for training, validation and testing respectively. The keymotivation ofMNRE is that, for each relational fact, the relation patterns in sentences of different languages should be substantially consistent, and MNRE can utilize the pattern consistency and complementarity among
languages to achieve better results for relation extraction. Nevertheless, these RE systems are insufficient due to the lack of training data. (2) MNRE(CNN)-R has similar performance as compared to MNRE(CNN) when the recall is low. 4 shows the precision-recall curves for each
method. The evaluation method assumes that if a RE system accurately finds more relational facts in KBs from the testing set, it will achieve better performance for relation extraction. . 61572273, 61532010), and the Key Technologies Research andDevelopment Program of China (No. (2010) model distant supervision for relation extraction as a multiinstance single-label learning problem, and Hoffmann et al. ( dc is the dimension of output embeddings of the convolution layer)
After that, we combines all local features via a max-pooling operation and apply a hyperbolic tangent function to obtain a fixed-sized sentence vector for the input sentence. 2014BAK04B03). (5)
Here rj is the query vector of the relation r with respect to the j-th language. Both the works focus on multi-lingual transfer learning and learn a predictive model on a new language for existing KBs, by leveraging unified representation learning for cross-lingual entities. It indicates that by jointly training with multi-lingual attention, both Chinese and English relation extractors are beneficial from those sentences from the other language. (2009) align plain text with Freebase to automatically generate training instances following the distant supervision assumption. Relation extraction typically classifies each entity pair into various relation types according to supporting sentences that the both entities appear, which needs human-labelled relationspecific training instances. On the basis of mono-lingual attention, cross-lingual attention
is capable of further removing unlikely sentences and resulting in more concentrated and informative sentences, with the factor of consistent correspondence of relation patterns among different languages. Consistency. Multi-lingual data will benefit relation extraction for the following two reasons: 1. We select CNN proposed in (Zeng et al., 2014) as our sentence encoder and implement it by ourselves which achieves comparable results as the authors reported on their experimental dataset NYT104. 