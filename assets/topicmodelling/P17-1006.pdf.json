Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for lowfrequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for language understanding systems, which may infer that inexpensive is a rephrasing for expensive or may not associate acquire with acquires. In this work, we propose a novel morph-fitting procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces. In intrinsic evaluation over four languages, we show that our approach: 1) improves low-frequency word estimates; and 2) boosts the semantic quality of the entire word vector collection. Instead, our method injects morphological constraints generated using simple language-specific rules, pulling inflectional forms of the same word close together and pushing derivational antonyms far apart. Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of morphology for tackling long-tail phenomena in language understanding tasks. The authors are grateful to the anonymous reviewers for their helpful suggestions. The explicit post-hoc injection of morphological constraints enables: a) the estimation of more accurate vectors for lowfrequency words which are linked to their highfrequency forms by the constructed constraints;1 this tackles the data sparsity problem; and b) specialising the distributional space to distinguish between similarity and relatedness (Kiela et al., 2015), thus supporting language understanding applications such as dialogue state tracking (DST).2
As a post-processor, morph-fitting allows the integration of morphological rules with any distributional vector space in any language: it treats an input distributional word vector space as a black box and fine-tunes it so that the transformed space reflects the knowledge coded in the input morphological constraints (e.g., Italian words rispettoso and irrispetosa should be far apart in the trans-
1For instance, the vector for the word katalanischem which occurs only 9 times in the German Wikipedia will be pulled closer to the more reliable vectors for katalanisch and katalanischer, with frequencies of 2097 and 1383 respectively. 2.1 The ATTRACT-REPEL Model  The ATTRACT-REPEL model, proposed by Mrkšić et al. Similar trends in results persist with d = 100, 500.
all of our intrinsic and extrinsic experiments. This pipelined approach results in a simpler, more portable model. (2016b), who formulate the idea of post-training specialisation in a generative Bayesian framework. The constraints emphasise similarity on one side (e.g., by extracting morphological synonyms), and antonymy on the other (by extracting morphological antonyms), see Fig. (2017b). 3). In this Wizard-of-Oz setup, two Amazon Mechanical Turk workers assumed the role of the user and the system asking/providing information about restaurants in Cambridge (operating over the same ontology and database used for DSTC2 (Henderson et al., 2014a)). Embedded Semantics: Morphology can encode semantic relations such as antonymy (e.g. cup and coffee) have a low rating. The key idea is to learn a morphological composition function (Lazaridou et al., 2013; Cotterell and Schütze, 2017) which synthesises the representation of a word given the representations of its constituent morphemes. The results with EN SGNS-LARGE vectors are shown in Fig. 2.1). This creates pairs such as (create, creating) and (create, created). Additional discrepancies between SimLex and downstream DST performance are detected for German and Italian. Comparison to Other Specialisation Methods We also tried using other post-processing specialisation models from the literature in lieu of ATTRACT-REPEL using the same set of “morphological” synonymy and antonymy constraints. The NBT model keeps word vectors fixed during training, so that unseen, yet related words can be mapped to the right intent at test time (e.g. English performance shows little variation across the four word vector collections investigated here. The majority of proposed architectures encode morphological information, provided either as gold standard morphological resources (SylakGlassman et al., 2015) such as CELEX (Baayen et al., 1995) or as an external analyser such as Morfessor (Creutz and Lagus, 2007), along with distributional information jointly at training time in the language modelling (LM) objective (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Schütze, 2015; Bhatia et al., 2016, i.a.). User: What’s the address? This is not always considered when techniques are evaluated on languages such as English or Chinese, which do not have rich morphology. 3a-3d show results for the morph-fitted SGNS-LARGE vectors. The second term pushes antonyms away from each other. An ontology consists of a number of slots and their assorted slot values. The results are summarised in Tab. These vectors have varying vocabulary coverage and are trained with different architectures. This finding confirms that the method is robust: its effectiveness does not depend on the architecture used to construct the initial space. 2Representation models that do not distinguish between synonyms and antonyms may have grave implications in downstream language understanding applications such as spoken dialogue systems: a user looking for ‘an affordable Chinese restaurant in west Cambridge’ does not want a recommendation for ‘an expensive Thai place in east Oxford’. 1 and Tab. 2. Their work uses gold morphological lexicons; we show that competitive performance can be achieved using a non-exhaustive set of simple rules. These are: (R1) if w1, w2 ∈Wen, where w2 = w1 + ing/ed/s, then add (w1, w2) and (w2, w1) to the set of ATTRACT constraints A. 3 also demon-
7Unlike other gold standard resources such as WordSim353 (Finkelstein et al., 2002) or MEN (Bruni et al., 2014), SimLex and SimVerb provided explicit guidelines to discern between semantic similarity and association, so that related but non-similar words (e.g. 3. 2.4 These constraints (Sect. The DST model is the first component of modern dialogue pipelines (Young, 2010). 5) and downstream tasks (see later Fig. In a restaurant search domain, sets of slot-values could include PRICE = [cheap, expensive] or FOOD = [Thai, Indian, ...]. Evaluation Setup The principal metric we use to measure DST performance is the joint goal accuracy, which represents the proportion of test set dialogue turns where all user goals expressed up to that point of the dialogue were decoded correctly (Henderson et al., 2014a). They clearly indicate that MFIT-AR outperforms the two other post-processors for each language. words from the in-batch ATTRACT constraints to be closer to one another than to any other word in the current mini-batch. Given the initial vector space and collections of ATTRACT and REPEL constraints A and R, the model gradually modifies the space to bring the designated word vectors closer together or further apart. Morph-fixed Vectors A baseline which utilises an equal amount of knowledge as morph-fitting, termed morph-fixing, fixes the vector of each word to the distributional vector of its most frequent inflectional synonym, tying the vectors of lowfrequency words to their more frequent inflections. Instead, it makes use of the observation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the
56
proliferation of word forms in morphologically rich languages. In our experiments, we investigate the extent to which morph-fitting improves DST performance, and whether these gains exhibit stronger correlation with intrinsic performance. inform(area=south, price=cheap) System: Seven Days is very popular. It is used by the downstream dialogue manager component to choose the subsequent system response (Su et al., 2016). The NBT learns to compose these vectors into intermediate utterance and context representations. This probability distribution over the possible dialogue states (defined by the domain ontology) is the system’s internal estimate of the user’s goals. This relaxation ensures a wider portability of ATTRACTREPEL to languages and domains without readily available or adequate resources. This corroborates our intuition that, as a morphologically simpler language, English stands to gain less from fine-tuning the morphological variation for downstream applications. The morph-fixed vectors do not enhance DST performance, probably because fixing word vectors to their highest frequency inflectional form eliminates useful semantic content encoded in the original vectors. Fig. 4 Intrinsic Evaluation: Word Similarity  Evaluation Setup and Datasets The first set of experiments intrinsically evaluates morph-fitted vector spaces on word similarity benchmarks, using Spearman’s rank correlation as the evaluation metric. These correspond to languages in the Multilingual SimLex-999 dataset. 3. In Italian, we rely on the sets of rules spanning: (1) regular formation of plural (libro / libri); (2) regular verb conjugation (aspettare / aspettiamo); (3) regular formation of past participle (aspettare / aspettato); and (4) rules regarding grammatical gender (bianco / bianca). 3.6 We label these collections of vectors SGNS-LARGE. is expressed at word level” (Tsarfaty et al., 2010), pose specific challenges for NLP. 6 Related Work  Semantic Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Future work will focus on other potential sources of morphological knowledge, porting the framework to other morphologically rich languages and downstream tasks, and on further refinements of the post-processing specialisation algorithm and the constraint selection. 2. In this work, we tackle the two challenges jointly by introducing a resource-light vector space finetuning procedure termed morph-fitting. This step yields additional constraints such as (rispettosa, irrispettosi) (see Fig. The first term pulls the ATTRACT examples (xl, xr) ∈ A closer together. Morphfitted vectors bring consistent improvement across all experiments, regardless of the quality of the initial distributional space. The Dialogue State Tracking Challenge (DSTC) shared task series formalised the evaluation and provided labelled DST datasets (Henderson et al., 2014a,b; Williams et al., 2016). We use this dataset for our multilingual evaluation.8
Morph-fitting EN Word Vectors As the first experiment, we morph-fit a wide spectrum of EN distributional vectors induced by various architectures (see Sect. IT and DE benefit from both kinds of morph-fitting: IT performance increases from 74.1→ 78.1 (MFIT-A) and DE performance rises even more: 60.6→ 66.3 (MFIT-AR), setting a new state-of-the-art score for both datasets. First, we use the SimLex-999 dataset, as well as SimVerb-3500, a recent EN verb pair similarity dataset providing similarity ratings for 3,500 verb pairs.7 SimLex-999 was translated to DE, IT, and RU by Leviant and Reichart (2015), and they crowdsourced similarity scores from native speakers. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014). In summary, we believe these results show that SimLex is not a perfect proxy for downstream performance in language understanding tasks. ReLU(x) = max(0, x) is the standard rectified linear unit (Nair and Hinton, 2010). However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological richness of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them to better reflect true semantic similarity, and learning more accurate representations for low-frequency words. On the other hand, we see the opposite trend in Italian, where the MFITA vectors score lower than the MFIT-AR vectors on SimLex, but higher on the DST task. Finally, we have shown that the use of morph-fitted vectors boosts the performance of downstream language understanding models which rely on word representations as features, especially for morphologically rich languages such as German. While we observe a slight drop in SimLex performance with the DE MFIT-AR vectors compared to the MFIT-A ones, their relative performance is reversed in the DST task. The results in intrinsic word similarity tasks show that morph-fitting improves vector spaces induced by distributional models across four languages. In future work, we will study how to fur-
ther refine extracted sets of constraints. 1). strates that the morph-fitted vector spaces consistently outperform the morph-fixed ones. (2017b), is an extension of the PARAGRAM procedure proposed by Wieting et al.   Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 56–68 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1006  1 Introduction  Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al.,
2011). In DSTC datasets, users would quickly adapt to the system’s inability to deal with complex queries. 5, while Fig. Naturally, introducing more sophisticated rules is possible in order to cover for other special cases and morphological irregularities (e.g., sweep / swept), but in all our EN experiments, A is based on the two simple EN rules R1 and R2. Morph-fitting falls into the latter category. . . We test standard distributional models: Common-Crawl GloVe (Pennington et al., 2014), SGNS vectors (Mikolov et al., 2013) with various contexts (BOW = bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. Another practical difference is that we focus on similarity and evaluate morph-fitting in a well-defined downstream task where the artefacts of the distributional hypothesis are known to prompt statistical system failures. Most prominent word representation techniques are grounded in the distributional hypothesis (Harris, 1954), relying on word co-occurrence information in large textual corpora (Curran, 2004; Turney and Pantel, 2010; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Schwartz et al., 2015, i.a.). Regardless, its performance does correlate with downstream performance to a large extent, providing a useful indicator for the usefulness of specific word vector
spaces for extrinsic tasks such as DST. The following example shows the true dialogue state in a multi-turn dialogue:
User: What’s good in the southern part of town? The other three languages, with more complicated morphology, yield a larger number of rules. Contrary to our work, these models typically coalesce all lexical relations. On the other hand, the latter refers to the formation of new words with semantic shifts in meaning (Schone and Jurafsky, 2001; Haspelmath and Sims, 2013; Lazaridou et al., 2013; Zeller et al., 2013; Cotterell and Schütze, 2017). We compare ATTRACT-REPEL to the retrofitting model
of (Faruqui et al., 2015) and counter-fitting (Mrkšić et al., 2017a). Second, counter-fitting is computationally intractable with SGNS-LARGE vectors, as its regularisation term involves the computation of all pairwise distances between words in the vocabulary. We then choose the word w′max from the set Ww1 with the maximum frequency in the training data, and fix all other word vectors in Ww1 to its word vector. 3a. 4. The full sets of rules are available as supplemental material. The NBT models for EN, DE and IT are trained using four variants of the SGNS-LARGE vectors: 1) the initial distributional vectors; 2) morph-fixed vectors; 3) and 4) the two variants of morph-fitted vectors (see Sect. The proposed method does not require curated knowledge bases or gold lexicons. Users typed instead of speaking, removing the need to deal with noisy speech recognition. These are then used to decide which of the ontology-defined intents (goals) have been expressed by the user. However, large gains on SimLex-999 do not always induce correspondingly large gains in downstream performance. On the other hand, lemmatising the entire corpus can obfuscate the differences that exist between different word forms even though they share some aspects of meaning. Conversely, the WOZ setup allowed them to use sophisticated language. For DE, we use another rule targeting suffix replacement: -voll is replaced by -los. . Results and Discussion The dark bars (against the right axes) in Fig. Further Discussion The simplicity of the used language-specific rules does come at a cost of occasionally generating incorrect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress). If w[: −1] is a function which strips the last character from word w, the second rule is: (R2)
if w1 ends with the letter e and w1 ∈ Wen and w2 ∈ Wen, where w2 = w1[: −1] + ing/ed, then add (w1, w2) and (w2, w1) to A. Naively treating each realisation as a separate word leads to sparsity problems and a failure to exploit their shared semantics. 1 illustrates the effects of morph-fitting by qualitative examples in three languages: the vast majority of nearest neighbours are “morphological” synonyms. The comparison between MFIT-A and MFITAR indicates that both sets of constraints are important for the fine-tuning process. As shown by Mrkšić et al. 2.2 Language-Specific Rules and Constraints  Semantic Specialisation with Constraints The fine-tuning ATTRACT-REPEL procedure is entirely driven by the input ATTRACT and REPEL sets of
5We use hyperparameter values δatt = 0.6, δrpl = 0.0, λreg = 10
−9 from prior work without fine-tuning. 8Since Leviant and Reichart (2015) re-scored the original EN SimLex, we use their EN SimLex version for consistency. This demonstrates that both types of constraints are useful for semantic specialisation. If w1, w2 ∈ Wen, where w2 is generated by adding a prefix from APen to w1, then (w1, w2) and (w2, w1) are added to the set of REPEL constraints R. This rule generates pairs such as (advantage, disadvantage) and (regular, irregular). We also test the symmetricpattern based vectors of Schwartz et al. We report an improvement of 4% on Italian, and 6% on German when using morph-fitted vectors instead of the distributional ones, setting a new state-of-theart DST performance for the two datasets.3
3There are no readily available DST datasets for Russian. Another class of models, operating at the character level, shares a similar methodology: such models compose token-level representations from subcomponent embeddings (subwords, morphemes, or characters) (dos Santos and Zadrozny, 2014; Ling et al., 2015; Cao and Rei, 2016; Kim et al., 2016;
Wieting et al., 2016; Verwimp et al., 2017, i.a.). Morphologically rich languages, in which “substantial grammatical information. We also experiment with standard well-known distributional spaces in other languages (IT and DE), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vulić and Korhonen, 2016a). The ‘negative’ example ti for each word xi in any ATTRACT pair is the word vector closest to xi among the examples in the current minibatch (distinct from its target synonym and xi itself). We hypothesise that the difference in performance mainly stems from context-sensitive vector space updates performed by ATTRACT-REPEL. In slot-based systems, application domains are specified using ontologies that define the search constraints which users can express. Tab. Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. In this work, we use translations of this dataset to Italian and German, released by Mrkšić et al. For instance, this generates an IT pair (rispettoso, irrispettoso) (see Fig. Acknowledgments  This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). Other Starting Distributional Vectors We also analyse the impact of morph-fitting on other collections of well-known EN word vectors. The final term of the cost function serves to retain the abundance of semantic information encoded in the starting distributional space. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). Conversely, the other two models perform pairwise updates which do not consider what effect each update has on the example pair’s relation to other word vectors (for a detailed comparison, see (Mrkšić et al., 2017b)). In spirit, our work is similar to Cotterell et al. Vocabularies Wen, Wde, Wit, Wru are compiled by retaining all word forms from the four Wikipedias with word frequency over 10, see Tab. Estimating Rare Words: A single lemma can have many different surface realisations. If (xl, xr) ∈ BR is the current minibatch of REPEL constraints, this term can be expressed as follows:
R(BR) = ∑
(xl,xr)∈BR
(ReLU (δrpl + xlxr − xltr)
+ ReLU (δrpl + xlxr − xrtr))
In this case, each word’s ‘negative’ example is the (in-batch) word vector furthest away from it (and distinct from the word’s target antonym). 2 Morph-fitting: Methodology  Preliminaries In this work, we focus on four languages with varying levels of morphological complexity: English (EN), German (DE), Italian (IT), and Russian (RU). Experiments on Other Languages We next extend our experiments to other languages, testing both morph-fitting variants. This choice is guided by our intent to fine-tune the original vector space in order to improve the embedded semantic relations. We then show that incorporating morph-fitted vectors into a state-of-the-art neural-network DST model results in improved tracking performance, especially for morphologically rich languages. What is more, the rules for DE, IT, and RU were created by non-native, non-fluent speakers with a limited knowledge of the three languages, exemplifying the simplicity and portability of the approach. literate and illiterate, expensive and inexpensive) or (near-)synonymy (north, northern, northerly). The method makes use of implicit semantic signals encoded in inflectional and derivational rules which describe the morphological processes in a language. MFIT-A yields consistent gains over the initial spaces, and (consistent) further improvements are achieved by also incorporating the antonymous REPEL constraints. inform(area=south, price=cheap); request(address) System: Seven Days is at 66 Regent Street. This rule yields pairs such as (look, looks), (look, looking), (look, looked). These scores confirm the effectiveness and robustness of morph-fitting across languages, suggesting that the idea of fitting to morphological constraints is indeed language-agnostic, given the set of languagespecific rule-based constraints. We demonstrate the efficacy of morph-fitting in four languages (English, German, Italian, Russian), yielding large and consistent improvements on benchmarking word similarity evaluation sets such as SimLex-999 (Hill et al., 2015), its multilingual extension (Leviant and Reichart, 2015), and SimVerb-3500 (Gerz et al., 2016). To illustrate the improvements, note that the best score on SimVerb for a model trained on running text is achieved by Context2vec (ρ = 0.388); injecting morphological constraints into this vector space results in a gain of 7.1 ρ points. The intuition is that we want antonymous words from the input REPEL constraints to be further away from each other than from any other word in the current mini-batch; δrpl is now the repel margin. 3). (2017). (2015). . We train all models for 10 epochs with AdaGrad (Duchi et al., 2011). successful and accomplished) and antonymy constraints (e.g. Following the same principle, we use APde = {un, nicht, anti, ir, in, miss}, APit = {in, ir, im, anti}, and APru = {не, анти}. This means that this term forces synonymous
4A native speaker can easily come up with these sets of morphological rules (or at least with a reasonable subset of them) without any linguistic training. We further expand the set of REPEL constraints by transitively combining antonymy pairs from the previous step with inflectional ATTRACT pairs. The results on SimLex and SimVerb are summarised in Tab. First, retrofitting is able to incorporate only synonymy/ATTRACT pairs, while our results demonstrate the usefulness of both types of constraints, both for intrinsic evaluation (Tab. Great hot pot. For the ATTRACT constraints, we focus on inflectional rather than on derivational morphology rules as the former preserve the full meaning of a word, modifying it only to reflect grammatical roles such as verb tense or case markers (e.g., (en_read, en_reads) or (de_katalanisch, de_katalanischer)). In the case of distributional vector space models, morphological complexity brings two challenges to the fore:
1. formed vector space, see Fig. While a plethora of DST models are available based on, e.g., handcrafted rules (Wang et al., 2014) or conditional random fields (Lee and Eskenazi, 2013), the recent DST methodology has seen a shift towards neural-
network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkšić et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkšić et al., 2017a, i.a.). RR is supported by the IntelICRI grant: Hybrid Models for Minimally Supervised Information Extraction from Conversations. It serves to capture the intents expressed by the user at each dialogue turn and update the belief state. The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkšić et al., 2017a). We define two rules for English, widely recognised as morphologically simple (Avramidis and Koehn, 2008; Cotterell et al., 2016b). Our framework facilitates the inclusion of antonyms at no extra cost and naturally extends to constraints from other sources (e.g., WordNet) in future work. This result again points at the discrepancy between intrinsic and extrinsic evaluation: the considerable gains in SimLex performance do not necessarily induce similar gains in downstream performance. An additional rule replaces the suffix -ful with -less, extracting antonyms such as (careful, careless). 2.2) are used as input for the vector space post-processing ATTRACT-REPEL algorithm (outlined in Sect. Formalised as an instance of the post-processing semantic specialisation paradigm (Faruqui et al., 2015; Mrkšić et al., 2016), morphfitting is steered by a set of linguistic constraints derived from simple language-specific rules which describe (a subset of) morphological processes in a language. 1). We induce 300-dimensional word vectors, with the frequency cut-off set to 10. We then extract sets of linguistic constraints from these (large) vocabularies using a set of simple language-specific if-then-else rules, see Tab. User: How about something cheaper? The morph-fixed vectors (MFIX) serve as our primary baseline, as they outperformed another straightforward baseline based on stemming across
6Other SGNS parameters were set to standard values (Baroni et al., 2014; Vulić and Korhonen, 2016b): 15 epochs, 15 negative samples, global learning rate: .025, subsampling rate: 1e− 4. The final A andR constraint counts are given in Tab. northern to north). Extracting ATTRACT Pairs The core difference between inflectional and derivational morphology can be summarised in a few lines as follows: the former refers to a set of processes through which the word form expresses meaningful syntactic information, e.g., verb tense, without any change to the semantics of the word. Besides these, another set of rules is used for German and Russian: (5) regular declension (e.g., asiatisch / asiatischem). 1). The WOZ 2.0 release expanded the dataset to 1,200 dialogues (Mrkšić et al., 2017a). inform(area=south) System: Vedanta is the top-rated Indian place. We use a standard set of EN “antonymy” prefixes: APen = {dis, il, un, in, im, ir, mis, non, anti} (Fromkin et al., 2013). We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). Morph-fitting Variants We analyse two variants of morph-fitting: (1) using ATTRACT constraints only (MFIT-A), and (2) using both ATTRACT and REPEL constraints (MFIT-AR). 7 Conclusion and Future Work  We have presented a novel morph-fitting method which injects morphological knowledge in the form of linguistic constraints into word vector spaces. constraints. Extracting REPEL Pairs As another source of implicit semantic signals, W also contains words which represent derivational antonyms: e.g., two words that denote concepts with opposite meanings, generated through a derivational process. On the other hand, morph-fitting makes use of this information, supplementing it with semantic relations between different morphological forms. These conclusions are in line with the SimLex gains, where morph-fitting outperforms both distributional and morph-fixed vectors. The improvements are reported for all four languages, and with a variety of input distributional spaces, verifying the robustness of the approach. 3 Experimental Setup  Training Data and Setup For each of the four languages we train the skip-gram with negative sampling (SGNS) model (Mikolov et al., 2013)
on the latest Wikipedia dump of each language. These can be extracted from a variety of semantic databases such as WordNet (Fellbaum, 1998), the Paraphrase Database (Ganitkevitch et al., 2013; Pavlick et al., 2015), or BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014) as done in prior work (Faruqui et al., 2015; Wieting et al., 2015; Mrkšić et al., 2016, i.a.). Word Vectors and Morphology The use of morphological resources to improve the representations of morphemes and words is an active area of research. The two baselines were trained for 20 iterations using suggested settings. The results for EN, DE, and IT are summarised in Fig. In this work, we investigate another option: extracting constraints without curated knowledge bases in a spectrum of languages by exploiting inherent language-specific properties related to linguistic morphology. nimble and clumsy) into pre-trained word vectors. 3 show the DST performance of NBT models making use of the four vector collections. (2017b), semantic specialisation of the employed word vectors ben-
efits DST performance across all three languages. Besides their lower performance, the two other specialisation models have additional disadvantages compared to the proposed morph-fitting model. 3). Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016). In contrast to prior work, our model decouples the use of morphological information, now provided in the form of inflectional and derivational rules transformed into constraints, from the actual training. If BA denotes the current mini-batch of ATTRACT examples, this term can be expressed as:
A(BA) = ∑
(xl,xr)∈BA
(ReLU (δatt + xltl − xlxr)
+ ReLU (δatt + xrtr − xlxr))
where δatt is the similarity margin which determines how much closer synonymous vectors should be to each other than to each of their respective negative examples. The vocabulary sizes |W | for each language are provided in Tab. Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkšić et al., 2016). For each word w1, we construct a set of M + 1 words Ww1 = {w1, w′1, . If xiniti is the initial distributional vector and V (B) is the set of all vectors present in the given mini-batch, this term (per mini-batch) is expressed as follows:
R(BA,BR) = ∑
xi∈V (BA∪BR) λreg
∥∥∥xiniti − xi ∥∥∥ 2
where λreg is the L2 regularisation constant.5 This term effectively pulls word vectors towards their initial (distributional) values, ensuring that relations encoded in initial vectors persist as long as they do not contradict the newly injected ones. Data: Multilingual WOZ 2.0 Dataset Our DST evaluation is based on the WOZ dataset, released by Wen et al. The method’s cost function consists of three terms. It provides a generic framework for incorporating similarity (e.g. , w′M} consisting of the word w1 itself and all M words which cooccur with w1 in the ATTRACT constraints. The key idea of the fine-tuning process is to pull synonymous examples described by the constraints closer together in the transformed vector space, while at the same time pushing antonymous examples away from each other. 2. 5 Downstream Task: Dialogue State Tracking (DST)  Goal-oriented dialogue systems provide conversational interfaces for tasks such as booking flights or finding restaurants. 