While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. Based on the assumption, our method is able to train a source-to-target NMT model (“student”) without parallel corpora available guided by an existing pivot-to-target NMT model (“teacher”) on a source-pivot parallel corpus. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs. Compared with pivot-based approaches (Cheng et al., 2016a), our method allows direct parameter estimation of the intended NMT model, without the need to divide decoding into two steps. Intuitively, the source-to-target translation can be indirectly modeled by bridging two NMT models via a pivot:
P (y|x;θx→z,θz→y) = ∑
z
P (z|x;θx→z)P (y|z;θz→y). Table 4 shows BLEU scores on the Europarl corpus of our proposed methods. However, pivot-based approaches usually need to divide the decoding process into two steps, which is not only more computationally expensive, but also potentially suffers from the error propagation problem (Zhu et al., 2013). They use parallel corpora of multiple languages to train one single model, which is then able to translate a language pair without parallel corpora available. Note that both methods depend on a source-pivot-target decoding path. tend to have lower validation loss compared with sentence-level methods. We use dashed lines to denote language pairs with parallel corpora available and solid lines with arrows to denote translation directions. For the Europarl corpus, we evaluate our proposed methods on Spanish-French (Es-Fr) and German-French (De-Fr) translation tasks in a
zero-resource scenario. Specifically, the size of the source-pivot corpus is orders of magnitude smaller than that of the pivot-target corpus. We also analyze zero-resource translation with small source-pivot data, and combine our wordlevel sampling method with initialization and parameter freezing suggested by (Zoph et al., 2016). As a result, a number of authors have endeavored to explore methods for translating language pairs without parallel corpora available. Cheng et al. In the following subsections, we will introduce two approaches to zero-resource neural machine translation based on the two assumptions. This idea is widely used in SMT (de Gispert and Mariño, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013). To further verify this approach, we conduct ex-
periments on the large scale WMT corpus for Spanish-French translation. We set k = 5 and α = 5×10−3. Kim and Rush (2016) claim a similar observation in data distillation for NMT and provide an explanation that student distributions are more peaked for sentence-level methods. This strategy not only improves efficiency but also avoids error propagation in decoding. In this paper, we propose a new method for zero-resource neural machine translation. To fulfill this task, we combine our best performing word-sampling method with the initialization and parameter freezing strategy proposed in (Zoph et al., 2016). To compare with pivotbased methods, we use the same dataset as (Cheng et al., 2016a). (2016b) utilize a much larger training set.5 Our method obtains significant improvement over the pivot baseline by +3.46 BLEU points on Newstest2012 and over many-to-one by +5.84 BLEU points on Newstest2013. Beam size with k = 1 (greedy decoding) and k = 5 are investigated in our experiments, denoted as sent-greedy and sent-beam, respectively.3
2. Figure 1(a) illustrates the basic idea of pivotbased approaches to zero-resource NMT (Cheng et al., 2016a). (2016a) propose pivotbased NMT by simultaneously improving sourceto-pivot and pivot-to-target translation quality in order to improve source-to-target translation quality. We significantly improve the baseline pivot method by +5.63 BLEU points and the state-ofthe-art transfer learning method by +0.56 BLEU points. Cheng et al. The Europarl corpus is used in the experiments. The two assumptions are empirically verified in our experiments (see Table 2). 3 Approach    3.1 Assumptions  In this work, we propose to directly model the intended source-to-target neural translation based on a teacher-student framework. We find that JSENT and JWORD decrease over time, suggesting that the source-to-target and pivot-totarget models do have small KL divergence at both sentence and word levels. (2016a) propose a pivot-based method for zeroresource NMT: it first translates the source language to a pivot language, which is then translated to the target language. We introduce sentence-level and word-level teaching to
guide the learning process of the student model. Kim and Rush (2016) first introduce knowledge distillation in neural machine translation. 61331013) and the 863 Program (2015AA015407). Our work is also related to knowledge distillation, which trains a compact model to approximate the function learned by a larger, more complex model or an ensemble of models (Bucila et al., 2006; Ba and Caurana, 2014; Li et al., 2014; Hinton et al., 2015). Note that the KL-divergence between two models is defined at the word level:
KL ( P (y|z,y<j ; θ̂z→y) ∣∣∣ ∣∣∣P (y|x,y<j ;θx→y) )
= ∑
y∈Vy P (y|z,y<j ; θ̂z→y) log
P (y|z,y<j ; θ̂z→y) P (y|x,y<j ;θx→y) ,
where Vy is the target vocabulary. Firat et al. The results are in line with our observation in Table 2 that sentence-level KL divergence by beam approximation is smaller than that by greedy approximation. (2016) report that NMT obtains much lower BLEU scores than SMT if only small-scale parallel corpora are available. Simple and easy-to-implement, pivot-based methods have been widely used in SMT for
translating zero-resource language pairs (de Gispert and Mariño, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013). To deal with out-of-vocabulary words, we adopt byte pair encoding (BPE) (Sennrich et al., 2016) to split words into sub-words. Random sampling brings a large variance (Sutskever et al., 2014; Ranzato et al., 2015; He et al., 2016) for sentence-level teaching. (2016) indicate that NMT obtains much worse translation quality than a statistical machine translation (SMT) system on low-resource languages. Another direction is to develop a universal NMT model in multilingual scenarios (Johnson et al., 2016; Ha et al., 2016). Another direction is to achieve source-to-target NMT without parallel data via a pivot, which is either text (Cheng et al., 2016a) or image (Nakayama and Nishida, 2016). Two variations of our framework are used in the exper-
1http://www.statmt.org/wmt07/shared-task.html 2dl4mt-tutorial: https://github.com/nyu-dl
iments:
1. One advantage of our approach is that Equation (1) can be used as the decision rule for decoding, which avoids the error propagation problem faced by two-step decoding in pivot-based approaches. Our results indicate that this is not necessarily the case: the sent-beam method converges to +0.31 BLEU points on the validation set with +13 validation loss compared with the word-beam method. In the future, we plan to test our approach on more diverse language pairs, e.g., zero-resource Uyghur-English translation using Chinese as a pivot. For word-level experiments, we observe that the word-sampling method performs much better than the other two methods: +1.94 BLEU points on Spanish-French translation and +1.88 BLEU points on German-French translation over the word-greedy method; +2.65 BLEU points on Spanish-French translation and +2.84 BLEU points on German-French translation over the word-beam method. However, as the
time complexity grows linearly with the number of beams k, the better performance is achieved at the expense of search time. Table 2 shows the results. Unlike pivot-based approaches, we first use the pivot-target parallel corpus Dz,y to obtain a teacher model P (y|z; θ̂z→y), where θ̂z→y is a set of learned model parameters. They use the multi-way NMT model trained by other language pairs to generate a pseudo parallel corpus and fine-tune the attention mechanism of the multiway NMT model to enable zero-resource translation. Sentence-Level Teaching: for simplicity, we use the mode as suggested in (Kim and Rush, 2016) to approximate the target sentence space in calculating the expected gradients with respect to the expectation in Equation (7). For the pivot and likelihood methods, the Spainish sentence segment ’sentáis al volante’ is lost when translated to English. (2)
As shown in Figure 1(a), pivot-based approaches assume that the source-pivot parallel corpus Dx,z and the pivot-target parallel corpus Dz,y are available.   Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1925–1935 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1176  1 Introduction  Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community. Table 5 shows the results of our word-sampling method in comparison with other state-of-the-art baselines. To avoid the trilingual corpus constituted by the source-pivot and pivottarget corpora, we split the overlapping pivot sentences of the original source-pivot and pivot-target corpora into two equal parts and merge them separately with the non-overlapping parts for each language pair. Zoph et
∗Corresponding author: Yang Liu. We find that both the sent-beam and word-sampling methods outperform the pivot-based approaches in a zero-resource scenario across language pairs. The results on test set for Eureparl Corpus are 32.24 BLEU over Spanish-French translation and 24.91 BLEU over German-French translation, which are slightly better than the sent-beam method. Although NMT has achieved state-of-the-art translation performance on resource-rich language pairs such as English-French and German-English (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016; Johnson et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low-resource languages. They suggest to generate a pseudo corpus to train the student network. The development and test sets are from WMT 2006 shared task.1 The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. (12)
Therefore, our goal is to find a set of source-totarget model parameters that minimizes the training objective:
θ̂x→y = argmin θx→y
{ JWORD(θx→y) } . This setting makes sense in applications. Firat et al. In addition, the word-sampling mothod surprisingly obtains improvement over the likelihood method, which leverages a source-target parallel corpus. Generally, models with lower validation loss tend to have higher BLEU. We observe that word-level models
4Validation loss: the average negative log-likelihood of sentence pairs on the validation set. Word-Level Teaching: we use the same mode approximation approach as in sentence-level teaching to approximate the expectation in Equation 12, denoted as word-greedy (beam search with k = 1) and word-beam (beam search with k = 5) respectively. Cheng et al. They fine-tune the attention part using pseudo bilingual sentences for the zeroresource language pair. See Table 1 for detailed statistics for the Europarl and WMT corpora. We use the same data preprocessing as (Cheng et al., 2016a). (2016b) propose an approach which delivers the multi-way, multilingual NMT model proposed by (Firat et al., 2016a) to zero-resource translation. All the experiments treat English as the pivot language and French as the target language. (2016b) present a multi-way, multilingual model with shared attention to achieve zeroresource translation. Let x be a source-language sentence and y be a target-language sentence. Therefore, both methods miss this information in the translated French sentence. However, a major difficulty faced by our approach is the intractability in calculating the gradients because of the exponential search space of target sentences. Table 6 shows translation examples of the pivot and likelihood methods proposed in (Cheng et al., 2016a) and our proposed word-sampling method. 4.5 Results with Small Source-Pivot Data  The word-sampling method can also be applied to zero-resource NMT with a small source-pivot corpus. All the sentences are tokenized by the tokenize.perl script. (Equation (9)) on 2,000 parallel source-pivot sentences from the development set of WMT 2006 shared task. (1)
As a data-driven approach, NMT heavily relies on the availability of large-scale parallel corpora to deliver state-of-the-art translation performance (Wu et al., 2016; Johnson et al., 2016). Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low-resource language pairs. The source-to-target model is randomly initialized at iteration 0. Then, the teacher model “teaches” the student model P (y|x;θx→y) on the source-pivot parallel corpus Dx,z based on the following assumptions. We set the size of GermanEnglish training data to 100K and use the same teacher model trained with 900K English-French sentences. Besides multilingual NMT, another important line of research is bridging source and target languages via a pivot language. As a data-driven approach, NMT treats parallel corpora as the major source for acquiring translation knowledge. Acknowledgments  This work was done while Yun Chen is visiting Tsinghua University. BPE is also used to reduce the vocabulary size. For k-best list, we renormalize the probabilities
P (y|z; θ̂z→y) ∼ P (y|z; θ̂z→y) α
∑ y∈Yk P (y|z; θ̂z→y)α ,
where Yk is the k-best list from beam search of the teacher model and α is a hyperparameter controling the sharpness of the distribution (Och, 2003). (7)
In training, our goal is to find a set of source-totarget model parameters that minimizes the training objective:
θ̂x→y = argmin θx→y
{ JSENT(θx→y) } . However, considering the traing time and the memory consumption, we think mode approximation is already a good way to approximate the target sentence space for sentence-level teaching. Table 7 gives the BLEU score of our method on German-French translation compared with three other methods. Therefore, the heavy dependence on the quantity of training data poses a severe challenge for NMT to translate zeroresource language pairs. Firat et al. 3.2 Sentence-Level Teaching  Given a source-pivot parallel corpus Dx,z , our training objective based on Assumption 1 is defined as follows:
JSENT(θx→y) = ∑
〈x,z〉∈Dx,z KL ( P (y|z; θ̂z→y) ∣∣∣ ∣∣∣P (y|x;θx→y) ) , (5)
where the KL divergence sums over all possible target sentences:
KL ( P (y|z; θ̂z→y) ∣∣∣ ∣∣∣P (y|x;θx→y) )
= ∑
y
P (y|z; θ̂z→y) log P (y|z; θ̂z→y) P (y|x;θx→y) . Zoph et al. 4.4 Results on the WMT Corpus  The word-sampling method obtains the best performance in our five proposed approaches according to experiments on the Europarl corpus. After obtaining θ̂x→y, the same decision rule as shown in Equation (1) can be utilized to find the most probable target sentence ŷ for a source sentence x. Given learned model parameters θ̂x→y, the decision rule for finding the translation with the highest probability for a source sentence x is given by
ŷ = argmax y
{ P (y|x; θ̂x→y) } . We can further introduce a word-level assumption:
Assumption 2 If a source sentence x is a translation of a pivot sentence z, then the probability of generating a target word y from x should be close to that from its counterpart z, given the already obtained partial translation y<j . Although these approaches prove to be effective, the combination of multiple languages in modeling and training leads to increased complexity compared with standard NMT. However, the word-sampling method generates ’volant sur’, which partially translates ’sentáis al volante’, resulting in improved translation quality of targetlanguage sentence. The experiments on the Europarl corpus show that our approach obtains an significant improvement over the pivot-based baseline. All the sentences are tokenized by the tokenize.perl script. (2016a) use the same datasets and the same preprocessing as ours. We use P (y|x;θx→y)
to denote a source-to-target neural translation model, where θx→y is a set of model parameters. This is indeed the case in our result: on German-French translation task the argmax for the sent-beam student model (on average) approximately accounts for 3.49% of the total probability mass, while the corresponding number is 1.25% for the word-beam student model and 2.60% for the teacher model. (6)
As the teacher model parameters are fixed, the training objective can be equivalently written as
JSENT(θx→y) = − ∑
〈x,z〉∈Dx,z Ey|z;θ̂z→y
[ logP (y|x;θx→y) ] . 5 Related Work  Training NMT models in a zero-resource scenario by leveraging other languages has attracted intensive attention in recent years. As it is impractical to enumerate all possible pivot sentences, the two NMT models are trained separately in practice:
θ̂x→z = argmax θx→z
{ ∑
〈x,z〉∈Dx,z logP (z|x;θx→z)
} ,
θ̂z→y = argmax θz→y
{ ∑
〈z,y〉∈Dz,y logP (y|z;θz→y)
} . al. To address this problem, it is possible to construct a sub-space by either sampling (Shen et al., 2016), generating a k-best list (Cheng et al., 2016b) or mode approximation (Kim and Rush, 2016). (4)
The above two-step decoding process potentially suffers from the error propagation problem (Zhu et al., 2013): the translation errors made in the first step (i.e., source-to-pivot translation) will affect the second step (i.e., pivot-to-target translation). We use case-sensitive BLEU to evaluate translation results. Assumption 1 If a source sentence x is a translation of a pivot sentence z, then the probability of generating a target sentence y from x should be close to that from its counterpart z. We leverage an open-source NMT toolkit dl4mt implemented by Theano 2 for all the experiments and compare our approach with state-of-the-art multilingual methods (Firat et al., 2016b) and pivot-based methods (Cheng et al., 2016a). We plot validation loss4 and BLEU scores over iterations on the German-French translation task in Figure 2. Surprisingly, our method outperforms all other methods. Experiments on the Europarl and WMT corpora across languages show that our proposed wordlevel sampling method can significantly outperforms the state-of-the-art pivot-based methods and multilingual methods in terms of translation quality and decoding efficiency. (13)
We use similar approaches as described in Section 3.2 for approximating the full search space with sentence-level teaching. These methods can be roughly divided into two broad categories: multilingual and pivot-based. Experiments on the Europarl and WMT datasets show that our approach achieves significant improvements in terms of both translation quality and decoding efficiency over a baseline pivot-based approach to zero-resource NMT on Spanish-French and German-French translation tasks. It is also interesting to extend the teacherstudent framework to other cross-lingual NLP applications as our method is transparent to architectures. Nakayama and Nishida (2016) achieve zeroresource machine translation by utilizing image as a pivot and training multimodal encoders to share common semantic representation. This work is partially supported by the National Natural Science Foundation of China (No.61522204, No. 4 Experiments    4.1 Setup  We evaluate our approach on the Europarl (Koehn, 2005) and WMT corpora. Newstest2011 serves as the development set and Newstest2012 and Newstest2013 serve as test sets. Let X, Y, and Z denote source, target, and pivot languages. Nakayama and Nishida (2016) show that using multimedia information as pivot also benefits zero-resource translation. 2 Background  Neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) advocates the use of neural networks to model the translation process in an end-to-end manner. 4.2 Assumptions Verification  To verify the assumptions in Section 3.1, we train a source-to-target translation model P (y|x;θx→y) and a pivot-to-target translation model P (y|z;θz→y) using the trilingual Europarl corpus. As pivotbased methods are agnostic to model structures, they have been adapted to NMT recently (Cheng et al., 2016a; Johnson et al., 2016). 6 Conclusion  In this paper, we propose a novel framework to train the student model without parallel corpora under the guidance of the pre-trained teacher model on a source-pivot parallel corpus. The
significant improvements can be explained by the error propagation problem of pivot-based methods that translation error of the source-to-pivot translation process is propagated to the pivot-to-target translation process. We run beam search on the pivot sentence with the teacher model and choose the highest-scoring target sentence as the mode. As the parameters of the teacher model are fixed, the training objective can be equivalently written as:
JWORD(θx→y) = − ∑
〈x,z〉∈Dx,z Ey|z;θ̂z→y
[ S(x,y, z, θ̂z→y,θx→y) ] , (11)
where
S(x,y, z, θ̂z→y,θx→y)
=
|y|∑
j=1
∑
y∈Vy P (y|z,y<j ; θ̂z→y)×
logP (y|x,y<j ;θx→y). The size of sub-words is set to 30K for each language. The size of sub-words is set to 43K, 33K, 43K for Spanish, English and French, respectively. Compared with their work, we focus on zero-resource learning instead of model compression. We combine the following corpora to form the Es-En and En-Fr parallel corpora: Common Crawl, News Commentary, Europarl v7 and UN. Several authors propose a universal encoderdecoder network in multilingual scenarios to perform zero-shot learning (Johnson et al., 2016; Ha et al., 2016). For sentencelevel approaches, the sent-beam method outperforms the sent-greedy method by +0.59 BLEU points over Spanish-French translation and +2.51 BLEU points over German-French translation on the test set. Then, standard stochastic gradient descent algorithms can be used to optimize model parameters. For the WMT corpus, we evaluate our approach on a Spanish-French (Es-Fr) translation task with a zero-resource setting. Given a source-target parallel corpus Dx,y, which is a set of parallel source-target sentences, the model parameters can be learned by maximizing the log-likelihood of the parallel corpus:
θ̂x→y = argmax θx→y
{ ∑
〈x,y〉∈Dx,y logP (y|x;θx→y)
} . Besides, Monte Carlo estimation by sampling from the teacher model is also investigated since it introduces more diverse data, denoted as wordsampling. 3.3 Word-Level Teaching  Instead of minimizing the KL divergence between the teacher and student models at the sentence level, we further define a training objective at the word level based on Assumption 2:
JWORD(θx→y) = ∑
〈x,z〉∈Dx,z Ey|z;θ̂z→y
[ J(x,y, z, θ̂z→y,θx→y) ] , (9)
where
J(x,y, z, θ̂z→y,θx→y)
=
|y|∑
j=1
KL ( P (y|z,y<j ; θ̂z→y) ∣∣∣ ∣∣∣
P (y|x,y<j ;θx→y) ) . For example, there are significantly fewer Urdu-English corpora available than
5Their training set does not include the Common Crawl corpus. English-French corpora. (10)
Equation (9) suggests that the teacher model P (y|z,y<j ; θ̂z→y) “teaches” the student model P (y|x,y<j ;θx→y) in a word-by-word way. Therefore, it is necessary to explore methods to directly model source-to-target translation without parallel corpora available. Note that our task is much harder than transfer learning (Zoph et al., 2016) since it depends on a parallel German-French corpus. Although Table 2 shows that word-level KL divergence approximated by sampling is larger than that by greedy or beam, sampling approximation introduces more data diversity for training, which dominates the effect of KL divergence difference. As shown in Figure 1(b), we still assume that a source-pivot parallel corpus Dx,z and a pivot-target parallel corpus Dz,y are available. Then, we measure the sentence-level and word-level KL divergence from the source-totarget model P (y|x;θx→y) at different iterations to the trained pivot-to-target model P (y|z; θ̂z→y) by caculating JSENT (Equation (5)) and JWORD
3We can also adopt sampling and k-best list for approximation. 4.3 Results on the Europarl Corpus  Table 3 gives BLEU scores on the Europarl corpus of our best performing sentence-level method (sent-beam) and word-level method (word-sampling) compared with pivot-based methods (Cheng et al., 2016a). (8)
With learned source-to-target model parameters θ̂x→y, we use the standard decision rule as shown in Equation (1) to find the translation ŷ for a source sentence x. The basic idea is to use a pre-trained pivot-to-target model (“teacher”) to guide the learning process of a source-to-target model (“student”) without training data available on a source-pivot parallel corpus. Due to the exponential search space of pivot sentences, the decoding process of translating an unseen source sentence x has to be divided into two steps:
ẑ = argmax z
{ P (z|x; θ̂x→z) } , (3)
ŷ = argmax y
{ P (y|ẑ; θ̂z→y) } . Our word-sampling method improves over the best performing zero-resource pivot-based method (soft) on Spanish-French translation by +3.29 BLEU points and German-French translation by +3.24 BLEU points. To train a source-to-target NMT model without parallel corpora (“student”), we leverage an existing pivot-to-target NMT model (“teacher”) to guide the learning process of the student model on a source-pivot parallel corpus. Our
1925
method assumes that parallel sentences should have close probabilities of generating a sentence in a third language. This universal model extracts translation knowledge from multiple different languages, making zero-resource translation feasible without direct training. 