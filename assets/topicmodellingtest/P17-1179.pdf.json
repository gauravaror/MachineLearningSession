Word embeddings are well known to capture linguistic regularities of the language on which they are trained. Researchers also observe that these regularities can transfer across languages. However, previous endeavors to connect separate monolingual word embeddings typically require cross-lingual signals as supervision, either in the form of parallel corpus or seed lexicon. Even though this task appears intrinsically cross-lingual, we are able to demonstrate encouraging performance without any cross-lingual clues. We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are crucial to successful training. In this work, we show that such cross-lingual connection can actually be established without any form of supervision. We carry out evaluation on the unsupervised bilingual lexicon induction task. The learned mapping function can then be used to translate each
source word x by finding the nearest target embedding to f (x). One possible reason is that the parameter search space Rd×d for the generator may still be too large. As the ground truth bilingual lexicon for evaluation, we use Chinese-English Translation Lexicon Version 3.0 (LDC2002L27). Then we propose three models that implement the game, and explore techniques to ensure the success of training. Therefore, we attribute the outcome to the difference in the loss and training techniques, but not the model architectures or data. (2016) address domain adaptation by adversarially training features to be domain invariant, and test on sentiment classification. More importantly, its learning mechanism is substantially different from ours. (2013a) use five thousand seed word translation pairs to train the linear transformation. We look into this aspect by varying the embedding dimension d in Figure 5. It is also about 20 times slower even though we utilize orthogonal parametrization instead of constrained optimization. Orthogonal Constraint  The above model is very difficult to train. This means a good transformation matrix is indeed
nearly orthogonal, and justifies our encouragement of G towards orthogonality. 61522204), the 973 Program (2014CB340501), and the National Natural Science Foundation of China (No. For Turkish, we utilize the preprocessing tools (tokenization and POS tagging) provided in LORELEI Language Packs (Strassel and Tracey, 2016), and its English side is preprocessed by NLTK. In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. Future work also includes investigating other divergences that adversarial training can minimize (Nowozin et al., 2016), and broader mathematical tools that match distributions (Mohamed and Lakshminarayanan, 2016). In this setting, our approach is able to considerably outperform both MonoGiza and the method by Cao et al. (2013a) observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages, as illustrated in Figure 1. In this case, we also observe the importance of the orthogonal constraint from the superiority of IA to TM, which supports our introduction of this constraint as we attempt zero supervision. 3 Training Techniques  Generative adversarial networks are notoriously difficult to train, and investigation into stabler training remains a research frontier (Radford et al., 2015; Salimans et al., 2016; Arjovsky and Bottou, 2017). This means a word translation has to be retrieved from a much larger pool of candidates. We use the most frequent S pairs for TM and IA. Although they need seed word translation pairs to train and thus not directly comparable, we report their performance with 50 and 100 seeds for reference. Table 3 lists some word translation examples given by the adversarial autoencoder model. We consider x to be drawn from a distribution px, and similarly y ∼ py. Finally, our evaluation on the bilingual lexicon induction task reveals encouraging performance, even though this task appears formidable without any cross-lingual supervision. For Turkish-English, we build a set of ground truth translation pairs in the same way as how we obtain seed word translation pairs from Google Translate, described above. Therefore we need a mechanism to select a good model. Effect of Embedding Dimension  As our approach takes monolingual word embeddings as input, it is conceivable that their quality significantly affects how well the two spaces can be connected by a linear map. Performance is measured by top-M accuracy (Vulić and Moens, 2013): If any of the M translations is found in the ground truth bilingual lexicon, the source word is considered to be handled correctly, and the accuracy is calculated as the percentage of correctly translated source words. The generator loss is given by
LG = − logD (Gx) . The imbalanced sizes of the Chinese and English Wikipedia do not seem to cause a problem for the structural isomorphism needed by our method. The agglutinative nature of Turkish can also add to the challenge. For these reasons, we consider the performance of our approach presented in Table 5 to be encouraging. 5.2 Adversarial Training  Generative adversarial networks are originally proposed for generating realistic images as an implicit generative model, but the adversarial training technique for matching distributions is generalizable to much more tasks, including natural
language processing. Results  As shown in Table 4, the MonoGiza baseline still does not work well on these language pairs, while our approach achieves much better performance. λ = 0 recovers the unidirectional transformation model, while larger λ should enforce a stricter orthogonal constraint. This can be implemented by two unidirectional models with a tied generator, as illustrated in Figure 2(b). The seed word translation pairs are obtained as follows. Our approach draws inspiration from recent advances in generative adversarial networks (Goodfellow et al., 2014). In the field of neural machine translation, a recent work (He et al., 2016) proposes dual learning, which also involves a two-agent game and therefore bears conceptual resemblance to the adversarial training idea. We find this automatically constructed bilingual lexicon to be noisier than the ones we use for the other language pairs; it often lists tens of translations for a source word. With the network
configuration used in our experiments, the adversarial autoencoder model takes about two hours to train for 500k minibatches on a single CPU. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vulić and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Kočiský et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. In contrast, our approach is immune from scalability issues by working with embeddings provided by word2vec, which is well known for its fast speed. • All forms of regularization help training by allowing us to liberally set the hidden layer size to a relatively large value. Acknowledgments  We thank the anonymous reviewers for their helpful comments. The success of our approach signifies the existence of universal lexical semantic structure across languages. Only then do seed-based methods catch up with our approach, and the performance difference is marginal even when more seeds are provided. MonoGiza obtains low performance, likely due to the harsh evaluation protocol (cf. In fact, if the generator is given sufficient capacity, it can in principle learn a constant mapping function to a target word embedding, which makes the discriminator impossible to distinguish, much like the “mode collapse” problem widely observed in the image domain (Radford et al., 2015; Salimans et al., 2016). As a generic binary classifier, a standard feedforward neural network with one hidden layer is used to parametrize the discriminator D, and its loss function is the usual cross-entropy loss, as in the value function (1):
LD = − logD (y)− log (1−D (Gx)) . The sharp drops can also be indicative in this case. We run the MonoGiza system as recommended by the toolkit.2 It can also utilize monolingual embeddings (Dou et al., 2015); in this case, we use the same embeddings as the input to our approach. Spanish English  et al. For example, Mikolov et al. This is unfortunate for low-resource languages and domains,
1959
because data encoding cross-lingual equivalence is often expensive to obtain. For the adversarial autoencoder model, λ = 1 generally works well, but λ = 10 appears stabler for the low-resource Turkish-English setting. The generator loss is given by
LG = − logD1 (Gx)− logD2 ( G>x ) . We therefore use it in our following experiments. 5 Related Work    5.1 Cross-Lingual Word Embeddings for Bilingual Lexicon Induction  Inducing bilingual lexica from non-parallel data is a long-standing cross-lingual task. Providing it with syntactic information can help (Dou and Knight, 2013), but in a lowresource scenario with zero cross-lingual information, parsers are likely to be inaccurate or even unavailable. This is achieved by matching the distributions of the transformed source language embeddings and target ones via adversarial training. Section 4.4). (2016b) had subsequent steps for their POS tagging task, it could be used for bilingual lexicon induction as well. After the generator G transforms a source word embedding x into a target language representation Gx, we should be able to reconstruct the source word embedding x by mapping back withG>. This research is also supported by the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiative and administered by the IDM Programme. 61331013). Although large-scale data may benefit the training of embeddings, it poses a greater challenge to bilingual lexicon induction. 4 Experiments  We evaluate the quality of the cross-lingual embedding transformation on the bilingual lexicon induction task. This indicates that too low a dimension hampers the encoding of linguistic information drawn from the corpus, and it is advisable to use a sufficiently large dimension. Previous works have attempted to constrain the transformation matrix to be orthogonal (Xing et al., 2015; Zhang et al., 2016b; Artetxe et al., 2016).   Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1959–1970 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1179  1 Introduction  As word is the basic unit of a language, the betterment of its representation has significant impact on various natural language processing tasks. Our approach does not make any assumptions and directly matches the mapped source embedding distribution with the target distribution by adversarial training. Figure 4 shows the accuracies with respect to
S. When the seeds are few, the seed-based methods exhibit clear performance degradation. With this finding, we can train for sufficient steps and save the model with the lowest generator loss. Soon following the success on monolingual tasks, the potential of word embeddings for crosslingual natural language processing has attracted much attention. This helps to ensure the translation quality. These methods are:
2http://www.isi.edu/naturallanguage/software/monogiza release v1.0.tar.gz
• Translation matrix (TM) (Mikolov et al., 2013a): the pioneer of this type of methods mentioned in the introduction, using linear transformation. Continuous word representations, commonly known as word embeddings, have formed the basis for numerous neural network models since their advent. 4.4 Comparison With (Cao et al., 2016)  In order to compare with the recent method by Cao et al. As we aim to find the cross-lingual transformation without supervision, it would be ideal to determine hyperparameters without a validation set. On top of input noise, hidden layer noise helps slightly. Unlike the other language pairs, the frequency cutoff threshold for Turkish-English is 100, as the amount of data is relatively small. We also simplify preprocessing by removing the noun restriction and the lemmatization step (cf. 3http://clic.cimec.unitn.it/˜georgiana.dinu/down 4https://translate.google.com  4.1 Experiments on Chinese-English    Data  For this set of experiments, the data for training word embeddings comes from Wikipedia comparable corpora.5 Following (Vulić and Moens, 2013), we retain only nouns with at least 1,000 occurrences. The last two models represent different relaxations of the orthogonal constraint, and the adversarial autoencoder model achieves the best performance. But imposing a strict orthogonal constraint hurts performance. We generally report the harshest top-1 accuracy, unless when comparing with published figures in Section 4.4. For example, Ganin et al. In this work, we aim to entirely eliminate the need for cross-lingual supervision. It encourages word embeddings from different languages to lie in the shared semantic space by matching the mean and variance of the hidden states, assumed to follow a Gaussian distribution, which is hard to justify. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). Our research deals with unsupervised bilingual lexicon induction based on word embeddings, and therefore works with word embedding distributions, which are more interpretable than the neural feature space of classifiers in the above works. However, connecting separate word embedding spaces typically requires supervision from crosslingual signals. A recent work also attempts adversarial training for cross-lingual embedding transformation (Barone, 2016). Adversarial training involves alternate gradient update of the generator and discriminator, which we implement with a simpler variant algorithm described in (Nowozin et al., 2016). Then the target translations are queried again and translated back to the source language, and those that do not match the original source words are discarded. The framework is carried out with reinforcement learning, and thus differs greatly in implementation from adversarial training. preprocessing decisions for the above experiments). Interestingly, the reconstruction loss LR and the value of ∥∥G>G− I ∥∥ F
exhibit synchronous drops, even if we use the unidirectional transformation model (λ = 0). This has far-reaching implication on low-resource scenarios (Daumé III and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013), because word embeddings only require plain text to train, which is the most abundant form of linguistic resource. (1)
Theoretical analysis reveals that adversarial training tries to minimize the Jensen-Shannon divergence JSD ( py||pf(x) ) (Goodfellow et al., 2014). 4.2 Experiments on Other Language Pairs    Data  We also induce bilingual lexica from Wikipedia comparable corpora for the following language pairs: Spanish-English, Italian-English, JapaneseChinese, and Turkish-English. Chen et al. • Additive Gaussian noise: ∼ N ( 0, σ2 ) . As the dimension increases, the accuracy improves and gradually levels off. For the Japanese corpus, we use MeCab10 for word segmentation and POS tagging. We train the CBOW model (Mikolov et al., 2013b) with default hyperparameters in word2vec.1 The embedding dimension d is 50 unless stated otherwise. Although Zhang et al. We therefore parametrize the generator as a transformation matrix G ∈ Rd×d. Their popularity results from the performance boost they bring, which should in turn be attributed to the linguistic regularities they capture (Mikolov et al., 2013b). We ensure the same input embeddings for these methods and ours. We therefore introduce the reconstruction loss measured by cosine similarity:
LR = − cos ( x,G>Gx ) . The accuracies are particularly high for SpanishEnglish and Italian-English, likely because they are closely related languages, and their embedding spaces may exhibit stronger isomorphism. Finally, but probably most importantly, the vocabularies expand dramatically compared to previous settings (see Table 1). This intuition can be formalized as the minimax game minGmaxD V (D,G) with value function
V (D,G)
=Ey∼py [logD (y)] + Ex∼px [log (1−D (G (x)))] . Recent advances in cross-lingual word embeddings (Vulić and Korhonen, 2016; Upadhyay et al.,
12As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. We tried the publicly available code on our data, but the results were not positive, either. G is
1https://code.google.com/archive/p/word2vec
initialized with a random orthogonal matrix. Sharing the underlying spirit with our approach, related methods also build upon monolingual word embeddings and find transformation to link different languages. The noise is typically additive Gaussian. We use a publicly available implementation.3
• Isometric alignment (IA) (Zhang et al., 2016b): an extension of TM by augmenting its learning objective with the isometric (orthogonal) constraint. The
9http://www.cis.uni-muenchen.de/˜schmid/tools/ TreeTagger
10http://taku910.github.io/mecab 11http://compling.hss.ntu.edu.sg/omw
performance on Japanese-Chinese is lower, on a comparable level with Chinese-English (cf. We therefore believe it is crucial to grant the generator with suitable capacity. It also means a linear transformation may be established to connect word embedding spaces, allowing word feature transfer. We contribute in this aspect by reporting techniques that are crucial to successful training for our task. We also tried non-linear maps parametrized by neural networks, without success. There are a total of 1,280 seed translation pairs for Chinese-English, which are removed from the test set during the evaluation for this experiment. 3.2 Model Selection  From a typical training trajectory shown in Figure 3, we observe that training is not convergent. As hinted by the isomorphism shown in Figure 1, previous works typically choose the mapping function f to be a linear map (Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015). With this term included, the loss function for the generator becomes
LG = − logD (Gx)− λ cos ( x,G>Gx ) , (6)
where λ is a hyperparameter that balances the two terms. In a recent study, Vulić and Korhonen (2016) show that at least hundreds of seed word translation pairs are needed for the model to generalize. For SpanishEnglish and Italian-English, we choose to use TreeTagger9 for preprocessing, as in (Vulić and Moens, 2013). (2016). We observe there are sharp drops of the generator loss LG, and find they correspond to good models, as the discriminator gets confused at these points with its classification accuracy (D accuracy) dropping simultaneously. Finally, in line with the finding in (Vulić and Korhonen, 2016), hundreds of seeds are needed for TM to generalize. Importantly, the minimization happens at the distribution level, without requiring word
translation pairs to supervise training. Comparison With Seed-Based Methods  In this section, we investigate how many seeds TM and IA require to attain the performance level of our approach. 3.1 Regularizing the Discriminator  Recently, it has been suggested to inject noise into the input to the discriminator (Sønderby et al.,
2016; Arjovsky and Bottou, 2017). 4.3 Large-Scale Settings  We experiment with large-scale Chinese-English data from two sources: the whole Wikipedia dump and Gigaword (LDC2011T13 and LDC2011T07). It modifies the objective for training embeddings, whereas our approach uses monolingual embeddings trained beforehand and held fixed. An orthogonal transformation is also theoretically appealing for its self-consistency (Smith et al., 2017) and numerical stability. In their pioneering work, Mikolov
∗Corresponding author. • Multiplicative Gaussian noise: ∼ N ( 1, σ2 ) . 2.2 Model 2: Bidirectional Transformation  The orthogonal parametrization is still quite slow. When sampling words for adversarial training, we penalize frequent words in a way similar to (Mikolov et al., 2013b). (5)
Note that this loss will be minimized if G is orthogonal. Adam (Kingma and Ba, 2014) is used as the optimizer, with default hyperparameters. This training difficulty motivates our orthogonal constraint. Turkish-English represents a low-resource scenario, and therefore the lexical semantic structure may be insufficiently captured by the embeddings. This lenient evaluation protocol should explain MonoGiza’s higher numbers in Table 6 than what we report in the other experiments. In the following experiments, we inject multiplicative Gaussian into the input and hidden layer of the discriminator with σ = 0.5. This interesting finding is in line with research on human cognition (Youn et al., 2016). The key intuition here is to find the mapping function to make f (x) seem to follow the distribution py, for all x ∼ px. Except for the decipherment approach, traditional statistical methods all require cross-lingual signals (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Vulić et al., 2011; Vulić and Moens, 2013). MonoGiza does not scale to such large vocabularies, as it already takes days to train in our Italian-English setting. 2016) have rekindled interest in bilingual lexicon induction. In fact, simply using the model saved at the end of training gives poor performance. This work is supported by the National Natural Science Foundation of China (No. Before feeding them into our system, we normalize the word embeddings to unit length. (3)
In line with previous work (Goodfellow et al., 2014), we find this loss easier to minimize than the original form log (1−D (Gx)). We first formulate our task in a fashion that naturally admits an adversarial game. From this point of view, we design an adversarial game as illustrated in Figure 2(a): The generator G implements the mapping function f , trying to make f (x) passable as target word embeddings, while the discriminator D is a binary classifier striving to distinguish between fake target word embeddings f (x) ∼ pf(x) and real ones y ∼ py. An exception is decipherment (Dou and Knight, 2012; Dou et al., 2015), and we use it as our baseline. (2)
For simplicity, here we write the loss with a minibatch size of 1; in our experiments we use 128. Apart from using top-5 accuracy as the evaluation metric, the ground truth bilingual lexicon is obtained by performing word alignment on a parallel corpus. For the Chinese side, we first use OpenCC6 to normalize characters to be simplified, and then perform Chinese word segmentation and POS tagging with THULAC.7 The preprocessing of the English side involves tokenization, POS tagging, lemmatization, and lowercasing, which we carry out with the NLTK toolkit.8 The statistics of the final training data is given in Table 1, along with the other experimental settings. After a source word embedding is transformed into the target space, its M nearest target embeddings (in terms of cosine similarity) are retrieved, and compared against the entry in a ground truth bilingual lexicon. As one of our baselines, the method by Cao et al. • Among the types of regularization, multiplicative Gaussian injected into the input is the most effective, and additive Gaussian is similar. If a hyperparameter configuration is poor, those values will oscillate without a clear drop. The hidden layer size ofD is 500. The model architectures are similar to ours, but the reported results are not positive. (2016), which also uses zero cross-lingual signal to connect monolingual embeddings, we replicate their French-English experiment to test our approach.12 This experimental setting has important differences from the above ones, mostly in the evaluation protocol. 3.3 Other Training Details  Our approach takes monolingual word embeddings as input. First, the degree of non-parallelism tends to increase. Overall Performance  Table 2 lists the performance of the MonoGiza baseline and our four variants of adversarial training. As noise injection is a form of regularization (Bishop, 1995; Van der Maaten et al., 2013; Wager et al., 2013), we also try l2 regularization, and directly restricting the hidden layer size to combat overfitting. Although this criterion is somewhat subjective, we find it to be quite feasible in practice. Our work also opens up opportunities for the processing of extremely low-resource languages and domains that lack parallel data completely. Our work is likely to benefit from advances in techniques that further stabilize adversarial training. 2.1 Model 1: Unidirectional Transformation  The first model directly implements the adversarial game, as shown in Figure 2(a). Baselines  Almost all approaches to bilingual lexicon induction from non-parallel data depend on seed lexica. Our findings include:
• Without regularization, it is not impossible for the optimizer to find a satisfactory parameter configuration, but the hidden layer size has to be tuned carefully. The ground truth bilingual lexica for SpanishEnglish and Italian-English are obtained from Open Multilingual WordNet11 through NLTK. For Japanese-Chinese, we use an in-house lexicon. This indicates that a balance of capacity between the generator and discriminator is needed. We can relax the orthogonal constraint and only require the transformation to be self-consistent (Smith et al., 2017): If G transforms the source word embedding space into the target language space, its transpose G> should transform the target language space back to the source. 6 Conclusion  In this work, we demonstrate the feasibility of connecting word embeddings of different languages without any cross-lingual signal. (2016) extend this idea to cross-lingual sentiment classification. Table 2), and these languages are relatively distantly related. 5http://linguatools.org/tools/corpora/wikipediacomparable-corpora
6https://github.com/BYVoid/OpenCC 7http://thulac.thunlp.org 8http://www.nltk.org
The unidirectional transformation model attains reasonable accuracy if trained successfully, but it is rather sensitive to hyperparameters and initialization. (4)  2.3 Model 3: Adversarial Autoencoder  As another way to relax the orthogonal constraint, we introduce the adversarial autoencoder (Makhzani et al., 2015), depicted in Figure 2(c). The decipherment approach is not based on distributional semantics, but rather views the source language as a cipher for the target language, and attempts to learn a statistical model to decipher the source language. First, we ask Google Translate4 to translate the source language vocabulary. Here we explore more possibilities, with the following types of noise, injected into the input and hidden layer:
• Multiplicative Bernoulli noise (dropout) (Srivastava et al., 2014): ∼ Bernoulli (p). (2016) also does not require cross-lingual signals to train bilingual word embeddings. Two separate discriminators are used, with the same cross-entropy loss as Equation (2) used by Model 1. However, using constrained optimization for our purpose is cumbersome, so we opt for an orthogonal parametrization (Mhammedi et al., 2016) of the generator instead. Finally, the translations are discarded if they fall out of our target language vocabulary. Our goal is to learn a mapping function f : Rd → Rd so that for a source word embedding x, f (x) lies close to the embedding of its target language translation y. 2 Models  In order to induce a bilingual lexicon, we start from two sets of monolingual word embeddings with dimensionality d. They are trained separately on two languages. Second, with cruder preprocessing, the noise in the corpora may take its toll. 